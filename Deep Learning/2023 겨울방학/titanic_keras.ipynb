{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('D:\\\\Dropbox\\\\GitHub\\\\deepstudy\\\\ko\\\\data\\\\train.csv')\n",
    "X_test = pd.read_csv('D:\\\\Dropbox\\\\GitHub\\\\deepstudy\\\\ko\\\\data\\\\test (1).csv')\n",
    "y_test = pd.read_csv('D:\\\\Dropbox\\\\GitHub\\\\deepstudy\\\\ko\\\\data\\\\y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Pclass = train.Pclass.astype('category')\n",
    "train.Embarked = train.Embarked.astype('category')\n",
    "\n",
    "train.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "test.Pclass = test.Pclass.astype('category')\n",
    "test.Embarked = test.Embarked.astype('category')\n",
    "\n",
    "test.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "train[['Age', 'Embarked']] = imputer.fit_transform(train[['Age', 'Embarked']])\n",
    "test[['Age', 'Embarked']] = imputer.transform(test[['Age', 'Embarked']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_train = pd.get_dummies(train[['Pclass', 'Sex', 'Embarked']])\n",
    "add_test = pd.get_dummies(test[['Pclass', 'Sex', 'Embarked']])\n",
    "\n",
    "train = pd.concat([train, add_train], axis=1)\n",
    "test = pd.concat([test, add_test], axis=1)\n",
    "\n",
    "train.drop(['Pclass', 'Sex', 'Embarked'], axis=1, inplace=True)\n",
    "test.drop(['Pclass', 'Sex', 'Embarked'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Survived', axis = 1)\n",
    "y = train.Survived\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y ,test_size = 0.2, random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 12) (179, 12) (712,) (179,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_21 (Dense)            (None, 300)               3900      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 300)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 300)               90300     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 300)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 301       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94,501\n",
      "Trainable params: 94,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(300, activation = 'relu', input_shape = (X_train.shape[1],)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(300, activation = 'relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# see model\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# save best model\n",
    "checkpoint = ModelCheckpoint(filepath = 'best_model.h5', monitor = 'val_loss', verbose = 1,\n",
    "                            save_best_only=True, model = 'max')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.Age = X_train.Age.astype('float32')\n",
    "X_train.Fare = X_train.Fare.astype('float32')\n",
    "X_val.Age = X_val.Age.astype('float32')\n",
    "X_val.Fare = X_val.Fare.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 1.0896 - accuracy: 0.6215 \n",
      "Epoch 1: val_loss improved from inf to 0.90493, saving model to best_model.h5\n",
      "23/23 [==============================] - 1s 15ms/step - loss: 1.1098 - accuracy: 0.6138 - val_loss: 0.9049 - val_accuracy: 0.6927\n",
      "Epoch 2/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.8187 - accuracy: 0.6618\n",
      "Epoch 2: val_loss improved from 0.90493 to 0.75643, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.8728 - accuracy: 0.6615 - val_loss: 0.7564 - val_accuracy: 0.6983\n",
      "Epoch 3/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.8422 - accuracy: 0.6641\n",
      "Epoch 3: val_loss improved from 0.75643 to 0.62668, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.8413 - accuracy: 0.6629 - val_loss: 0.6267 - val_accuracy: 0.6592\n",
      "Epoch 4/500\n",
      "13/23 [===============>..............] - ETA: 0s - loss: 0.7051 - accuracy: 0.6490\n",
      "Epoch 4: val_loss improved from 0.62668 to 0.58677, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.7409 - accuracy: 0.6531 - val_loss: 0.5868 - val_accuracy: 0.7207\n",
      "Epoch 5/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.6996 - accuracy: 0.7070\n",
      "Epoch 5: val_loss did not improve from 0.58677\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.6595 - accuracy: 0.7191 - val_loss: 0.7547 - val_accuracy: 0.7151\n",
      "Epoch 6/500\n",
      "12/23 [==============>...............] - ETA: 0s - loss: 0.6090 - accuracy: 0.6979\n",
      "Epoch 6: val_loss did not improve from 0.58677\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.5883 - accuracy: 0.7022 - val_loss: 0.5891 - val_accuracy: 0.7598\n",
      "Epoch 7/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.6225 - accuracy: 0.7078\n",
      "Epoch 7: val_loss did not improve from 0.58677\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6250 - accuracy: 0.7065 - val_loss: 0.9429 - val_accuracy: 0.6536\n",
      "Epoch 8/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.6691 - accuracy: 0.7135\n",
      "Epoch 8: val_loss did not improve from 0.58677\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6648 - accuracy: 0.7107 - val_loss: 0.7711 - val_accuracy: 0.7486\n",
      "Epoch 9/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.6083 - accuracy: 0.7396\n",
      "Epoch 9: val_loss improved from 0.58677 to 0.55095, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.6026 - accuracy: 0.7416 - val_loss: 0.5510 - val_accuracy: 0.7765\n",
      "Epoch 10/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5910 - accuracy: 0.7351\n",
      "Epoch 10: val_loss improved from 0.55095 to 0.53466, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5949 - accuracy: 0.7289 - val_loss: 0.5347 - val_accuracy: 0.7486\n",
      "Epoch 11/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.6336 - accuracy: 0.7451\n",
      "Epoch 11: val_loss did not improve from 0.53466\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.6126 - accuracy: 0.7444 - val_loss: 0.5654 - val_accuracy: 0.7933\n",
      "Epoch 12/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.5171 - accuracy: 0.7569\n",
      "Epoch 12: val_loss did not improve from 0.53466\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.5177 - accuracy: 0.7570 - val_loss: 0.5443 - val_accuracy: 0.7654\n",
      "Epoch 13/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.5136 - accuracy: 0.7582\n",
      "Epoch 13: val_loss improved from 0.53466 to 0.51234, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4988 - accuracy: 0.7711 - val_loss: 0.5123 - val_accuracy: 0.7821\n",
      "Epoch 14/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.4700 - accuracy: 0.7891\n",
      "Epoch 14: val_loss did not improve from 0.51234\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4881 - accuracy: 0.7781 - val_loss: 0.5485 - val_accuracy: 0.7542\n",
      "Epoch 15/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.4883 - accuracy: 0.7911\n",
      "Epoch 15: val_loss did not improve from 0.51234\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4888 - accuracy: 0.7907 - val_loss: 0.6077 - val_accuracy: 0.7709\n",
      "Epoch 16/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4992 - accuracy: 0.7768\n",
      "Epoch 16: val_loss improved from 0.51234 to 0.49704, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.5011 - accuracy: 0.7739 - val_loss: 0.4970 - val_accuracy: 0.7709\n",
      "Epoch 17/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.5092 - accuracy: 0.7757\n",
      "Epoch 17: val_loss did not improve from 0.49704\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4996 - accuracy: 0.7795 - val_loss: 0.5736 - val_accuracy: 0.7709\n",
      "Epoch 18/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.5077 - accuracy: 0.7917\n",
      "Epoch 18: val_loss did not improve from 0.49704\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.5118 - accuracy: 0.7907 - val_loss: 0.5126 - val_accuracy: 0.8045\n",
      "Epoch 19/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.4816 - accuracy: 0.7757\n",
      "Epoch 19: val_loss did not improve from 0.49704\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4787 - accuracy: 0.7809 - val_loss: 0.6181 - val_accuracy: 0.7486\n",
      "Epoch 20/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.5110 - accuracy: 0.7812\n",
      "Epoch 20: val_loss did not improve from 0.49704\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.5061 - accuracy: 0.7851 - val_loss: 0.5180 - val_accuracy: 0.7877\n",
      "Epoch 21/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4862 - accuracy: 0.7887\n",
      "Epoch 21: val_loss did not improve from 0.49704\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4813 - accuracy: 0.7935 - val_loss: 0.5881 - val_accuracy: 0.7654\n",
      "Epoch 22/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.4908 - accuracy: 0.7886\n",
      "Epoch 22: val_loss did not improve from 0.49704\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4875 - accuracy: 0.7851 - val_loss: 0.5563 - val_accuracy: 0.7598\n",
      "Epoch 23/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4882 - accuracy: 0.7783\n",
      "Epoch 23: val_loss did not improve from 0.49704\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4840 - accuracy: 0.7823 - val_loss: 0.5801 - val_accuracy: 0.7542\n",
      "Epoch 24/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.4599 - accuracy: 0.8219\n",
      "Epoch 24: val_loss improved from 0.49704 to 0.49110, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4690 - accuracy: 0.8160 - val_loss: 0.4911 - val_accuracy: 0.8212\n",
      "Epoch 25/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.4897 - accuracy: 0.7969\n",
      "Epoch 25: val_loss did not improve from 0.49110\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4851 - accuracy: 0.7935 - val_loss: 0.5081 - val_accuracy: 0.7989\n",
      "Epoch 26/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4430 - accuracy: 0.7961\n",
      "Epoch 26: val_loss did not improve from 0.49110\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4500 - accuracy: 0.7949 - val_loss: 0.4980 - val_accuracy: 0.7989\n",
      "Epoch 27/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.4480 - accuracy: 0.8062\n",
      "Epoch 27: val_loss did not improve from 0.49110\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4559 - accuracy: 0.8020 - val_loss: 0.5188 - val_accuracy: 0.8101\n",
      "Epoch 28/500\n",
      "14/23 [=================>............] - ETA: 0s - loss: 0.3954 - accuracy: 0.8393\n",
      "Epoch 28: val_loss did not improve from 0.49110\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4392 - accuracy: 0.8118 - val_loss: 0.5482 - val_accuracy: 0.7654\n",
      "Epoch 29/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.4506 - accuracy: 0.8016\n",
      "Epoch 29: val_loss did not improve from 0.49110\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4536 - accuracy: 0.7935 - val_loss: 0.5001 - val_accuracy: 0.7709\n",
      "Epoch 30/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.4353 - accuracy: 0.8207\n",
      "Epoch 30: val_loss improved from 0.49110 to 0.48397, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4540 - accuracy: 0.8132 - val_loss: 0.4840 - val_accuracy: 0.8212\n",
      "Epoch 31/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.4595 - accuracy: 0.7862\n",
      "Epoch 31: val_loss did not improve from 0.48397\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4522 - accuracy: 0.7865 - val_loss: 0.4874 - val_accuracy: 0.8045\n",
      "Epoch 32/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.4303 - accuracy: 0.8254\n",
      "Epoch 32: val_loss did not improve from 0.48397\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4417 - accuracy: 0.8160 - val_loss: 0.4945 - val_accuracy: 0.8212\n",
      "Epoch 33/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4632 - accuracy: 0.8051\n",
      "Epoch 33: val_loss did not improve from 0.48397\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4604 - accuracy: 0.8034 - val_loss: 0.5359 - val_accuracy: 0.7821\n",
      "Epoch 34/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.4450 - accuracy: 0.8073\n",
      "Epoch 34: val_loss improved from 0.48397 to 0.48228, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4385 - accuracy: 0.8132 - val_loss: 0.4823 - val_accuracy: 0.8045\n",
      "Epoch 35/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.4627 - accuracy: 0.7878\n",
      "Epoch 35: val_loss did not improve from 0.48228\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4547 - accuracy: 0.7935 - val_loss: 0.6038 - val_accuracy: 0.7709\n",
      "Epoch 36/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.4804 - accuracy: 0.7969\n",
      "Epoch 36: val_loss did not improve from 0.48228\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4745 - accuracy: 0.8006 - val_loss: 0.5117 - val_accuracy: 0.7989\n",
      "Epoch 37/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4362 - accuracy: 0.8139\n",
      "Epoch 37: val_loss did not improve from 0.48228\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4364 - accuracy: 0.8132 - val_loss: 0.5041 - val_accuracy: 0.7933\n",
      "Epoch 38/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.4264 - accuracy: 0.8188\n",
      "Epoch 38: val_loss did not improve from 0.48228\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4287 - accuracy: 0.8146 - val_loss: 0.4882 - val_accuracy: 0.8268\n",
      "Epoch 39/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4287 - accuracy: 0.8146\n",
      "Epoch 39: val_loss did not improve from 0.48228\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4287 - accuracy: 0.8146 - val_loss: 0.4827 - val_accuracy: 0.8101\n",
      "Epoch 40/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.4196 - accuracy: 0.8203\n",
      "Epoch 40: val_loss did not improve from 0.48228\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4327 - accuracy: 0.8146 - val_loss: 0.4893 - val_accuracy: 0.8101\n",
      "Epoch 41/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.4279 - accuracy: 0.8062\n",
      "Epoch 41: val_loss improved from 0.48228 to 0.46787, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4272 - accuracy: 0.8048 - val_loss: 0.4679 - val_accuracy: 0.8268\n",
      "Epoch 42/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.4222 - accuracy: 0.8180\n",
      "Epoch 42: val_loss did not improve from 0.46787\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4236 - accuracy: 0.8188 - val_loss: 0.4861 - val_accuracy: 0.7989\n",
      "Epoch 43/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.4472 - accuracy: 0.8062\n",
      "Epoch 43: val_loss improved from 0.46787 to 0.46398, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4482 - accuracy: 0.8104 - val_loss: 0.4640 - val_accuracy: 0.8212\n",
      "Epoch 44/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.4473 - accuracy: 0.8003\n",
      "Epoch 44: val_loss did not improve from 0.46398\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4422 - accuracy: 0.8048 - val_loss: 0.4678 - val_accuracy: 0.8324\n",
      "Epoch 45/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.4048 - accuracy: 0.8266\n",
      "Epoch 45: val_loss did not improve from 0.46398\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4179 - accuracy: 0.8188 - val_loss: 0.4753 - val_accuracy: 0.8156\n",
      "Epoch 46/500\n",
      "15/23 [==================>...........] - ETA: 0s - loss: 0.4075 - accuracy: 0.8062\n",
      "Epoch 46: val_loss did not improve from 0.46398\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4075 - accuracy: 0.8048 - val_loss: 0.4662 - val_accuracy: 0.8380\n",
      "Epoch 47/500\n",
      "13/23 [===============>..............] - ETA: 0s - loss: 0.4374 - accuracy: 0.8005\n",
      "Epoch 47: val_loss did not improve from 0.46398\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4403 - accuracy: 0.8104 - val_loss: 0.4671 - val_accuracy: 0.8324\n",
      "Epoch 48/500\n",
      "12/23 [==============>...............] - ETA: 0s - loss: 0.4039 - accuracy: 0.8047\n",
      "Epoch 48: val_loss did not improve from 0.46398\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4277 - accuracy: 0.8020 - val_loss: 0.4647 - val_accuracy: 0.8212\n",
      "Epoch 49/500\n",
      "12/23 [==============>...............] - ETA: 0s - loss: 0.3841 - accuracy: 0.8255\n",
      "Epoch 49: val_loss did not improve from 0.46398\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4085 - accuracy: 0.8160 - val_loss: 0.4756 - val_accuracy: 0.8045\n",
      "Epoch 50/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.4012 - accuracy: 0.8333\n",
      "Epoch 50: val_loss improved from 0.46398 to 0.46017, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4037 - accuracy: 0.8301 - val_loss: 0.4602 - val_accuracy: 0.8324\n",
      "Epoch 51/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.4289 - accuracy: 0.8033\n",
      "Epoch 51: val_loss improved from 0.46017 to 0.45510, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4165 - accuracy: 0.8104 - val_loss: 0.4551 - val_accuracy: 0.8212\n",
      "Epoch 52/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.4124 - accuracy: 0.8107\n",
      "Epoch 52: val_loss did not improve from 0.45510\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4110 - accuracy: 0.8132 - val_loss: 0.4652 - val_accuracy: 0.8324\n",
      "Epoch 53/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4214 - accuracy: 0.8188\n",
      "Epoch 53: val_loss did not improve from 0.45510\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4214 - accuracy: 0.8188 - val_loss: 0.4740 - val_accuracy: 0.8045\n",
      "Epoch 54/500\n",
      "14/23 [=================>............] - ETA: 0s - loss: 0.3894 - accuracy: 0.8304\n",
      "Epoch 54: val_loss did not improve from 0.45510\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.4086 - accuracy: 0.8202 - val_loss: 0.4652 - val_accuracy: 0.8101\n",
      "Epoch 55/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.4051 - accuracy: 0.8141\n",
      "Epoch 55: val_loss did not improve from 0.45510\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4036 - accuracy: 0.8146 - val_loss: 0.4703 - val_accuracy: 0.8268\n",
      "Epoch 56/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4133 - accuracy: 0.8104\n",
      "Epoch 56: val_loss did not improve from 0.45510\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4133 - accuracy: 0.8104 - val_loss: 0.4605 - val_accuracy: 0.8212\n",
      "Epoch 57/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3980 - accuracy: 0.8203\n",
      "Epoch 57: val_loss did not improve from 0.45510\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4008 - accuracy: 0.8244 - val_loss: 0.4774 - val_accuracy: 0.8212\n",
      "Epoch 58/500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.5538 - accuracy: 0.6875\n",
      "Epoch 58: val_loss did not improve from 0.45510\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3901 - accuracy: 0.8244 - val_loss: 0.4664 - val_accuracy: 0.8268\n",
      "Epoch 59/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.4018 - accuracy: 0.8281\n",
      "Epoch 59: val_loss did not improve from 0.45510\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4012 - accuracy: 0.8216 - val_loss: 0.4596 - val_accuracy: 0.8436\n",
      "Epoch 60/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4025 - accuracy: 0.8139\n",
      "Epoch 60: val_loss did not improve from 0.45510\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3999 - accuracy: 0.8146 - val_loss: 0.4799 - val_accuracy: 0.7933\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4027 - accuracy: 0.8146\n",
      "Epoch 61: val_loss did not improve from 0.45510\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4027 - accuracy: 0.8146 - val_loss: 0.4686 - val_accuracy: 0.8156\n",
      "Epoch 62/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.4209 - accuracy: 0.8125\n",
      "Epoch 62: val_loss did not improve from 0.45510\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4103 - accuracy: 0.8160 - val_loss: 0.4681 - val_accuracy: 0.8156\n",
      "Epoch 63/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3986 - accuracy: 0.8158\n",
      "Epoch 63: val_loss did not improve from 0.45510\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3910 - accuracy: 0.8188 - val_loss: 0.4569 - val_accuracy: 0.8212\n",
      "Epoch 64/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.4037 - accuracy: 0.8224\n",
      "Epoch 64: val_loss did not improve from 0.45510\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4013 - accuracy: 0.8244 - val_loss: 0.4573 - val_accuracy: 0.8268\n",
      "Epoch 65/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.4101 - accuracy: 0.8172\n",
      "Epoch 65: val_loss improved from 0.45510 to 0.45060, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3945 - accuracy: 0.8258 - val_loss: 0.4506 - val_accuracy: 0.8380\n",
      "Epoch 66/500\n",
      " 8/23 [=========>....................] - ETA: 0s - loss: 0.4103 - accuracy: 0.7891\n",
      "Epoch 66: val_loss did not improve from 0.45060\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3985 - accuracy: 0.8160 - val_loss: 0.4608 - val_accuracy: 0.8212\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3991 - accuracy: 0.8202\n",
      "Epoch 67: val_loss did not improve from 0.45060\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3991 - accuracy: 0.8202 - val_loss: 0.4644 - val_accuracy: 0.8101\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.4098 - accuracy: 0.8104\n",
      "Epoch 68: val_loss did not improve from 0.45060\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4098 - accuracy: 0.8104 - val_loss: 0.4619 - val_accuracy: 0.8268\n",
      "Epoch 69/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3912 - accuracy: 0.8180\n",
      "Epoch 69: val_loss did not improve from 0.45060\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3934 - accuracy: 0.8174 - val_loss: 0.4593 - val_accuracy: 0.8268\n",
      "Epoch 70/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3955 - accuracy: 0.8125\n",
      "Epoch 70: val_loss did not improve from 0.45060\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3931 - accuracy: 0.8174 - val_loss: 0.4598 - val_accuracy: 0.8547\n",
      "Epoch 71/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3857 - accuracy: 0.8217\n",
      "Epoch 71: val_loss did not improve from 0.45060\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3991 - accuracy: 0.8174 - val_loss: 0.4553 - val_accuracy: 0.8101\n",
      "Epoch 72/500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.5711 - accuracy: 0.8125\n",
      "Epoch 72: val_loss did not improve from 0.45060\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.4055 - accuracy: 0.8132 - val_loss: 0.4949 - val_accuracy: 0.7933\n",
      "Epoch 73/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3955 - accuracy: 0.8313\n",
      "Epoch 73: val_loss did not improve from 0.45060\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3985 - accuracy: 0.8272 - val_loss: 0.4515 - val_accuracy: 0.8156\n",
      "Epoch 74/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3995 - accuracy: 0.8155\n",
      "Epoch 74: val_loss did not improve from 0.45060\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3954 - accuracy: 0.8146 - val_loss: 0.4583 - val_accuracy: 0.8268\n",
      "Epoch 75/500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.2604 - accuracy: 0.8125\n",
      "Epoch 75: val_loss did not improve from 0.45060\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3872 - accuracy: 0.8272 - val_loss: 0.4678 - val_accuracy: 0.8045\n",
      "Epoch 76/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3768 - accuracy: 0.8351\n",
      "Epoch 76: val_loss did not improve from 0.45060\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3790 - accuracy: 0.8357 - val_loss: 0.4538 - val_accuracy: 0.8324\n",
      "Epoch 77/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3862 - accuracy: 0.8185\n",
      "Epoch 77: val_loss did not improve from 0.45060\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3877 - accuracy: 0.8160 - val_loss: 0.4508 - val_accuracy: 0.8212\n",
      "Epoch 78/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3873 - accuracy: 0.8289\n",
      "Epoch 78: val_loss did not improve from 0.45060\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3923 - accuracy: 0.8272 - val_loss: 0.4547 - val_accuracy: 0.8268\n",
      "Epoch 79/500\n",
      "10/23 [============>.................] - ETA: 0s - loss: 0.4309 - accuracy: 0.7937\n",
      "Epoch 79: val_loss improved from 0.45060 to 0.44732, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.4068 - accuracy: 0.8090 - val_loss: 0.4473 - val_accuracy: 0.8324\n",
      "Epoch 80/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3968 - accuracy: 0.8240\n",
      "Epoch 80: val_loss improved from 0.44732 to 0.44591, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3926 - accuracy: 0.8301 - val_loss: 0.4459 - val_accuracy: 0.8324\n",
      "Epoch 81/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3880 - accuracy: 0.8212\n",
      "Epoch 81: val_loss did not improve from 0.44591\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3838 - accuracy: 0.8230 - val_loss: 0.4517 - val_accuracy: 0.8268\n",
      "Epoch 82/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3774 - accuracy: 0.8310\n",
      "Epoch 82: val_loss did not improve from 0.44591\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3779 - accuracy: 0.8315 - val_loss: 0.4529 - val_accuracy: 0.8212\n",
      "Epoch 83/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3704 - accuracy: 0.8500\n",
      "Epoch 83: val_loss did not improve from 0.44591\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3771 - accuracy: 0.8427 - val_loss: 0.4532 - val_accuracy: 0.8212\n",
      "Epoch 84/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3781 - accuracy: 0.8199\n",
      "Epoch 84: val_loss did not improve from 0.44591\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3813 - accuracy: 0.8160 - val_loss: 0.4512 - val_accuracy: 0.8045\n",
      "Epoch 85/500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.3061 - accuracy: 0.8750\n",
      "Epoch 85: val_loss did not improve from 0.44591\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3823 - accuracy: 0.8174 - val_loss: 0.4493 - val_accuracy: 0.8436\n",
      "Epoch 86/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3803 - accuracy: 0.8327\n",
      "Epoch 86: val_loss did not improve from 0.44591\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3894 - accuracy: 0.8258 - val_loss: 0.4476 - val_accuracy: 0.8268\n",
      "Epoch 87/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3870 - accuracy: 0.8203\n",
      "Epoch 87: val_loss did not improve from 0.44591\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3822 - accuracy: 0.8188 - val_loss: 0.4533 - val_accuracy: 0.8212\n",
      "Epoch 88/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3735 - accuracy: 0.8177\n",
      "Epoch 88: val_loss did not improve from 0.44591\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3757 - accuracy: 0.8216 - val_loss: 0.4483 - val_accuracy: 0.8268\n",
      "Epoch 89/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3820 - accuracy: 0.8253\n",
      "Epoch 89: val_loss did not improve from 0.44591\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3821 - accuracy: 0.8244 - val_loss: 0.4491 - val_accuracy: 0.8436\n",
      "Epoch 90/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.3921 - accuracy: 0.8145\n",
      "Epoch 90: val_loss did not improve from 0.44591\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3756 - accuracy: 0.8244 - val_loss: 0.4496 - val_accuracy: 0.8268\n",
      "Epoch 91/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3871 - accuracy: 0.8259\n",
      "Epoch 91: val_loss improved from 0.44591 to 0.43927, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3806 - accuracy: 0.8301 - val_loss: 0.4393 - val_accuracy: 0.8436\n",
      "Epoch 92/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3851 - accuracy: 0.8254\n",
      "Epoch 92: val_loss did not improve from 0.43927\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3823 - accuracy: 0.8272 - val_loss: 0.4567 - val_accuracy: 0.8212\n",
      "Epoch 93/500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.2978 - accuracy: 0.8438\n",
      "Epoch 93: val_loss did not improve from 0.43927\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3800 - accuracy: 0.8287 - val_loss: 0.4449 - val_accuracy: 0.8324\n",
      "Epoch 94/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3782 - accuracy: 0.8297\n",
      "Epoch 94: val_loss did not improve from 0.43927\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3751 - accuracy: 0.8287 - val_loss: 0.4442 - val_accuracy: 0.8324\n",
      "Epoch 95/500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.3737 - accuracy: 0.8125\n",
      "Epoch 95: val_loss did not improve from 0.43927\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3713 - accuracy: 0.8230 - val_loss: 0.4484 - val_accuracy: 0.8101\n",
      "Epoch 96/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3774 - accuracy: 0.8258\n",
      "Epoch 96: val_loss did not improve from 0.43927\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3774 - accuracy: 0.8258 - val_loss: 0.4493 - val_accuracy: 0.8324\n",
      "Epoch 97/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3809 - accuracy: 0.8188\n",
      "Epoch 97: val_loss did not improve from 0.43927\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3791 - accuracy: 0.8230 - val_loss: 0.4748 - val_accuracy: 0.8101\n",
      "Epoch 98/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3895 - accuracy: 0.8259\n",
      "Epoch 98: val_loss did not improve from 0.43927\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3862 - accuracy: 0.8258 - val_loss: 0.4418 - val_accuracy: 0.8436\n",
      "Epoch 99/500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.3412 - accuracy: 0.8438\n",
      "Epoch 99: val_loss did not improve from 0.43927\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3846 - accuracy: 0.8287 - val_loss: 0.4511 - val_accuracy: 0.8324\n",
      "Epoch 100/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3743 - accuracy: 0.8257\n",
      "Epoch 100: val_loss did not improve from 0.43927\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3711 - accuracy: 0.8315 - val_loss: 0.4574 - val_accuracy: 0.8268\n",
      "Epoch 101/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3717 - accuracy: 0.8346\n",
      "Epoch 101: val_loss did not improve from 0.43927\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3697 - accuracy: 0.8301 - val_loss: 0.4478 - val_accuracy: 0.8324\n",
      "Epoch 102/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3687 - accuracy: 0.8299\n",
      "Epoch 102: val_loss did not improve from 0.43927\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3688 - accuracy: 0.8301 - val_loss: 0.4532 - val_accuracy: 0.8436\n",
      "Epoch 103/500\n",
      " 7/23 [========>.....................] - ETA: 0s - loss: 0.3666 - accuracy: 0.8393\n",
      "Epoch 103: val_loss did not improve from 0.43927\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3816 - accuracy: 0.8230 - val_loss: 0.4426 - val_accuracy: 0.8436\n",
      "Epoch 104/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3791 - accuracy: 0.8329\n",
      "Epoch 104: val_loss did not improve from 0.43927\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3791 - accuracy: 0.8329 - val_loss: 0.4484 - val_accuracy: 0.8380\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3742 - accuracy: 0.8202\n",
      "Epoch 105: val_loss did not improve from 0.43927\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3742 - accuracy: 0.8202 - val_loss: 0.4424 - val_accuracy: 0.8324\n",
      "Epoch 106/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3664 - accuracy: 0.8214\n",
      "Epoch 106: val_loss did not improve from 0.43927\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3632 - accuracy: 0.8272 - val_loss: 0.4406 - val_accuracy: 0.8436\n",
      "Epoch 107/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3807 - accuracy: 0.8244\n",
      "Epoch 107: val_loss improved from 0.43927 to 0.43845, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3750 - accuracy: 0.8272 - val_loss: 0.4384 - val_accuracy: 0.8324\n",
      "Epoch 108/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3608 - accuracy: 0.8388\n",
      "Epoch 108: val_loss did not improve from 0.43845\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3729 - accuracy: 0.8244 - val_loss: 0.4435 - val_accuracy: 0.8101\n",
      "Epoch 109/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3678 - accuracy: 0.8234\n",
      "Epoch 109: val_loss did not improve from 0.43845\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3693 - accuracy: 0.8244 - val_loss: 0.4610 - val_accuracy: 0.8492\n",
      "Epoch 110/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3874 - accuracy: 0.8264\n",
      "Epoch 110: val_loss did not improve from 0.43845\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3779 - accuracy: 0.8371 - val_loss: 0.4451 - val_accuracy: 0.8101\n",
      "Epoch 111/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3653 - accuracy: 0.8355\n",
      "Epoch 111: val_loss did not improve from 0.43845\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3616 - accuracy: 0.8357 - val_loss: 0.4560 - val_accuracy: 0.8324\n",
      "Epoch 112/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3715 - accuracy: 0.8363\n",
      "Epoch 112: val_loss did not improve from 0.43845\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3655 - accuracy: 0.8357 - val_loss: 0.4604 - val_accuracy: 0.8268\n",
      "Epoch 113/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3698 - accuracy: 0.8304\n",
      "Epoch 113: val_loss did not improve from 0.43845\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3747 - accuracy: 0.8287 - val_loss: 0.4440 - val_accuracy: 0.8212\n",
      "Epoch 114/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3712 - accuracy: 0.8266\n",
      "Epoch 114: val_loss did not improve from 0.43845\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3652 - accuracy: 0.8287 - val_loss: 0.4473 - val_accuracy: 0.8268\n",
      "Epoch 115/500\n",
      "14/23 [=================>............] - ETA: 0s - loss: 0.3539 - accuracy: 0.8549\n",
      "Epoch 115: val_loss did not improve from 0.43845\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3636 - accuracy: 0.8427 - val_loss: 0.4421 - val_accuracy: 0.8324\n",
      "Epoch 116/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3661 - accuracy: 0.8333\n",
      "Epoch 116: val_loss did not improve from 0.43845\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3614 - accuracy: 0.8399 - val_loss: 0.4585 - val_accuracy: 0.8324\n",
      "Epoch 117/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3705 - accuracy: 0.8316\n",
      "Epoch 117: val_loss did not improve from 0.43845\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3715 - accuracy: 0.8329 - val_loss: 0.4463 - val_accuracy: 0.8380\n",
      "Epoch 118/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3608 - accuracy: 0.8366\n",
      "Epoch 118: val_loss did not improve from 0.43845\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3583 - accuracy: 0.8385 - val_loss: 0.4500 - val_accuracy: 0.8268\n",
      "Epoch 119/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3664 - accuracy: 0.8324\n",
      "Epoch 119: val_loss did not improve from 0.43845\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3654 - accuracy: 0.8329 - val_loss: 0.4699 - val_accuracy: 0.8436\n",
      "Epoch 120/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3583 - accuracy: 0.8240\n",
      "Epoch 120: val_loss did not improve from 0.43845\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3678 - accuracy: 0.8174 - val_loss: 0.4450 - val_accuracy: 0.8268\n",
      "Epoch 121/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3711 - accuracy: 0.8267\n",
      "Epoch 121: val_loss improved from 0.43845 to 0.43090, saving model to best_model.h5\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3705 - accuracy: 0.8272 - val_loss: 0.4309 - val_accuracy: 0.8547\n",
      "Epoch 122/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3717 - accuracy: 0.8143\n",
      "Epoch 122: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3634 - accuracy: 0.8244 - val_loss: 0.4508 - val_accuracy: 0.8436\n",
      "Epoch 123/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.3653 - accuracy: 0.8281\n",
      "Epoch 123: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3695 - accuracy: 0.8329 - val_loss: 0.4579 - val_accuracy: 0.8268\n",
      "Epoch 124/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3480 - accuracy: 0.8368\n",
      "Epoch 124: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3594 - accuracy: 0.8329 - val_loss: 0.4486 - val_accuracy: 0.8324\n",
      "Epoch 125/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3676 - accuracy: 0.8273\n",
      "Epoch 125: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3648 - accuracy: 0.8258 - val_loss: 0.4514 - val_accuracy: 0.8380\n",
      "Epoch 126/500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.3873 - accuracy: 0.8750\n",
      "Epoch 126: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3604 - accuracy: 0.8301 - val_loss: 0.4630 - val_accuracy: 0.8268\n",
      "Epoch 127/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3558 - accuracy: 0.8328\n",
      "Epoch 127: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3582 - accuracy: 0.8287 - val_loss: 0.4484 - val_accuracy: 0.8380\n",
      "Epoch 128/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3695 - accuracy: 0.8290\n",
      "Epoch 128: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3653 - accuracy: 0.8272 - val_loss: 0.4547 - val_accuracy: 0.8324\n",
      "Epoch 129/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3500 - accuracy: 0.8466\n",
      "Epoch 129: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3499 - accuracy: 0.8455 - val_loss: 0.4692 - val_accuracy: 0.8212\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3620 - accuracy: 0.8301\n",
      "Epoch 130: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3620 - accuracy: 0.8301 - val_loss: 0.4628 - val_accuracy: 0.8380\n",
      "Epoch 131/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3559 - accuracy: 0.8313\n",
      "Epoch 131: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3732 - accuracy: 0.8244 - val_loss: 0.4678 - val_accuracy: 0.8212\n",
      "Epoch 132/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3760 - accuracy: 0.8257\n",
      "Epoch 132: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3752 - accuracy: 0.8301 - val_loss: 0.4631 - val_accuracy: 0.8436\n",
      "Epoch 133/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3700 - accuracy: 0.8224\n",
      "Epoch 133: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3676 - accuracy: 0.8244 - val_loss: 0.4590 - val_accuracy: 0.8380\n",
      "Epoch 134/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3482 - accuracy: 0.8385\n",
      "Epoch 134: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3654 - accuracy: 0.8315 - val_loss: 0.4612 - val_accuracy: 0.8156\n",
      "Epoch 135/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3644 - accuracy: 0.8338\n",
      "Epoch 135: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3624 - accuracy: 0.8343 - val_loss: 0.4607 - val_accuracy: 0.8380\n",
      "Epoch 136/500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.3261 - accuracy: 0.8438\n",
      "Epoch 136: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3557 - accuracy: 0.8371 - val_loss: 0.4565 - val_accuracy: 0.8268\n",
      "Epoch 137/500\n",
      " 7/23 [========>.....................] - ETA: 0s - loss: 0.3484 - accuracy: 0.8616\n",
      "Epoch 137: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3601 - accuracy: 0.8385 - val_loss: 0.4463 - val_accuracy: 0.8156\n",
      "Epoch 138/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3603 - accuracy: 0.8310\n",
      "Epoch 138: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3627 - accuracy: 0.8301 - val_loss: 0.4557 - val_accuracy: 0.8436\n",
      "Epoch 139/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3719 - accuracy: 0.8229\n",
      "Epoch 139: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3642 - accuracy: 0.8329 - val_loss: 0.4500 - val_accuracy: 0.8380\n",
      "Epoch 140/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3604 - accuracy: 0.8253\n",
      "Epoch 140: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3588 - accuracy: 0.8272 - val_loss: 0.4532 - val_accuracy: 0.8268\n",
      "Epoch 141/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3732 - accuracy: 0.8375\n",
      "Epoch 141: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3676 - accuracy: 0.8385 - val_loss: 0.4571 - val_accuracy: 0.8324\n",
      "Epoch 142/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3566 - accuracy: 0.8427\n",
      "Epoch 142: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3566 - accuracy: 0.8427 - val_loss: 0.4595 - val_accuracy: 0.8380\n",
      "Epoch 143/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3463 - accuracy: 0.8401\n",
      "Epoch 143: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3543 - accuracy: 0.8343 - val_loss: 0.4649 - val_accuracy: 0.8436\n",
      "Epoch 144/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3671 - accuracy: 0.8382\n",
      "Epoch 144: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3646 - accuracy: 0.8343 - val_loss: 0.4600 - val_accuracy: 0.8212\n",
      "Epoch 145/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.3378 - accuracy: 0.8594\n",
      "Epoch 145: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3574 - accuracy: 0.8413 - val_loss: 0.4501 - val_accuracy: 0.8212\n",
      "Epoch 146/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3602 - accuracy: 0.8289\n",
      "Epoch 146: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3565 - accuracy: 0.8287 - val_loss: 0.4578 - val_accuracy: 0.8380\n",
      "Epoch 147/500\n",
      "14/23 [=================>............] - ETA: 0s - loss: 0.3311 - accuracy: 0.8482\n",
      "Epoch 147: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3510 - accuracy: 0.8427 - val_loss: 0.4732 - val_accuracy: 0.8380\n",
      "Epoch 148/500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.3587 - accuracy: 0.8750\n",
      "Epoch 148: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.3521 - accuracy: 0.8301 - val_loss: 0.4583 - val_accuracy: 0.8380\n",
      "Epoch 149/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3484 - accuracy: 0.8472\n",
      "Epoch 149: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3480 - accuracy: 0.8441 - val_loss: 0.4898 - val_accuracy: 0.8212\n",
      "Epoch 150/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3551 - accuracy: 0.8391\n",
      "Epoch 150: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3516 - accuracy: 0.8385 - val_loss: 0.4770 - val_accuracy: 0.8492\n",
      "Epoch 151/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3492 - accuracy: 0.8524\n",
      "Epoch 151: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3520 - accuracy: 0.8511 - val_loss: 0.4728 - val_accuracy: 0.8268\n",
      "Epoch 152/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3482 - accuracy: 0.8381\n",
      "Epoch 152: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3469 - accuracy: 0.8385 - val_loss: 0.4646 - val_accuracy: 0.8380\n",
      "Epoch 153/500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.4405 - accuracy: 0.8438\n",
      "Epoch 153: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3537 - accuracy: 0.8343 - val_loss: 0.4787 - val_accuracy: 0.8268\n",
      "Epoch 154/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3464 - accuracy: 0.8344\n",
      "Epoch 154: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3470 - accuracy: 0.8343 - val_loss: 0.4642 - val_accuracy: 0.8547\n",
      "Epoch 155/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3477 - accuracy: 0.8420\n",
      "Epoch 155: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3496 - accuracy: 0.8399 - val_loss: 0.4681 - val_accuracy: 0.8436\n",
      "Epoch 156/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3489 - accuracy: 0.8364\n",
      "Epoch 156: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3445 - accuracy: 0.8371 - val_loss: 0.4693 - val_accuracy: 0.8380\n",
      "Epoch 157/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3657 - accuracy: 0.8316\n",
      "Epoch 157: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3572 - accuracy: 0.8371 - val_loss: 0.4629 - val_accuracy: 0.8101\n",
      "Epoch 158/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3607 - accuracy: 0.8406\n",
      "Epoch 158: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3591 - accuracy: 0.8441 - val_loss: 0.4549 - val_accuracy: 0.8324\n",
      "Epoch 159/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3603 - accuracy: 0.8352\n",
      "Epoch 159: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3599 - accuracy: 0.8357 - val_loss: 0.4581 - val_accuracy: 0.8324\n",
      "Epoch 160/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3479 - accuracy: 0.8368\n",
      "Epoch 160: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3450 - accuracy: 0.8413 - val_loss: 0.4767 - val_accuracy: 0.8436\n",
      "Epoch 161/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3352 - accuracy: 0.8509\n",
      "Epoch 161: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3382 - accuracy: 0.8483 - val_loss: 0.4762 - val_accuracy: 0.8101\n",
      "Epoch 162/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3463 - accuracy: 0.8438\n",
      "Epoch 162: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3559 - accuracy: 0.8399 - val_loss: 0.4712 - val_accuracy: 0.7989\n",
      "Epoch 163/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3545 - accuracy: 0.8399\n",
      "Epoch 163: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3545 - accuracy: 0.8399 - val_loss: 0.4814 - val_accuracy: 0.8324\n",
      "Epoch 164/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3422 - accuracy: 0.8493\n",
      "Epoch 164: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3524 - accuracy: 0.8385 - val_loss: 0.4751 - val_accuracy: 0.8547\n",
      "Epoch 165/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3438 - accuracy: 0.8438\n",
      "Epoch 165: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3497 - accuracy: 0.8441 - val_loss: 0.4654 - val_accuracy: 0.8268\n",
      "Epoch 166/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3614 - accuracy: 0.8289\n",
      "Epoch 166: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3544 - accuracy: 0.8329 - val_loss: 0.4636 - val_accuracy: 0.8324\n",
      "Epoch 167/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3658 - accuracy: 0.8351\n",
      "Epoch 167: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3469 - accuracy: 0.8399 - val_loss: 0.4805 - val_accuracy: 0.8268\n",
      "Epoch 168/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3514 - accuracy: 0.8309\n",
      "Epoch 168: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3414 - accuracy: 0.8343 - val_loss: 0.4853 - val_accuracy: 0.8547\n",
      "Epoch 169/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3426 - accuracy: 0.8339\n",
      "Epoch 169: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3549 - accuracy: 0.8329 - val_loss: 0.4918 - val_accuracy: 0.8324\n",
      "Epoch 170/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3452 - accuracy: 0.8438\n",
      "Epoch 170: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3467 - accuracy: 0.8441 - val_loss: 0.4757 - val_accuracy: 0.8380\n",
      "Epoch 171/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3404 - accuracy: 0.8388\n",
      "Epoch 171: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3453 - accuracy: 0.8427 - val_loss: 0.4928 - val_accuracy: 0.8380\n",
      "Epoch 172/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3463 - accuracy: 0.8393\n",
      "Epoch 172: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3501 - accuracy: 0.8343 - val_loss: 0.4684 - val_accuracy: 0.8268\n",
      "Epoch 173/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3497 - accuracy: 0.8469\n",
      "Epoch 173: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3396 - accuracy: 0.8553 - val_loss: 0.4725 - val_accuracy: 0.8324\n",
      "Epoch 174/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3435 - accuracy: 0.8405\n",
      "Epoch 174: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3417 - accuracy: 0.8427 - val_loss: 0.4819 - val_accuracy: 0.8324\n",
      "Epoch 175/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.3495 - accuracy: 0.8418\n",
      "Epoch 175: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3406 - accuracy: 0.8441 - val_loss: 0.4823 - val_accuracy: 0.8380\n",
      "Epoch 176/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3345 - accuracy: 0.8438\n",
      "Epoch 176: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3475 - accuracy: 0.8385 - val_loss: 0.4749 - val_accuracy: 0.8380\n",
      "Epoch 177/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3512 - accuracy: 0.8344\n",
      "Epoch 177: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3456 - accuracy: 0.8399 - val_loss: 0.4887 - val_accuracy: 0.8492\n",
      "Epoch 178/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3390 - accuracy: 0.8438\n",
      "Epoch 178: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3379 - accuracy: 0.8441 - val_loss: 0.4931 - val_accuracy: 0.8324\n",
      "Epoch 179/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3508 - accuracy: 0.8328\n",
      "Epoch 179: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3393 - accuracy: 0.8399 - val_loss: 0.4841 - val_accuracy: 0.8436\n",
      "Epoch 180/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3349 - accuracy: 0.8497\n",
      "Epoch 180: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3375 - accuracy: 0.8469 - val_loss: 0.4812 - val_accuracy: 0.8268\n",
      "Epoch 181/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3390 - accuracy: 0.8363\n",
      "Epoch 181: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3384 - accuracy: 0.8357 - val_loss: 0.4962 - val_accuracy: 0.8380\n",
      "Epoch 182/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3461 - accuracy: 0.8344\n",
      "Epoch 182: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3417 - accuracy: 0.8371 - val_loss: 0.4944 - val_accuracy: 0.8045\n",
      "Epoch 183/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3379 - accuracy: 0.8253\n",
      "Epoch 183: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3371 - accuracy: 0.8258 - val_loss: 0.4852 - val_accuracy: 0.8380\n",
      "Epoch 184/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3420 - accuracy: 0.8500\n",
      "Epoch 184: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3394 - accuracy: 0.8469 - val_loss: 0.4700 - val_accuracy: 0.8436\n",
      "Epoch 185/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3485 - accuracy: 0.8467\n",
      "Epoch 185: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3412 - accuracy: 0.8497 - val_loss: 0.4774 - val_accuracy: 0.8436\n",
      "Epoch 186/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3326 - accuracy: 0.8566\n",
      "Epoch 186: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.3316 - accuracy: 0.8511 - val_loss: 0.5019 - val_accuracy: 0.8268\n",
      "Epoch 187/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3493 - accuracy: 0.8453\n",
      "Epoch 187: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3456 - accuracy: 0.8413 - val_loss: 0.4917 - val_accuracy: 0.8268\n",
      "Epoch 188/500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.3133 - accuracy: 0.8438\n",
      "Epoch 188: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3441 - accuracy: 0.8385 - val_loss: 0.4996 - val_accuracy: 0.7877\n",
      "Epoch 189/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3204 - accuracy: 0.8553\n",
      "Epoch 189: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3152 - accuracy: 0.8567 - val_loss: 0.5181 - val_accuracy: 0.8436\n",
      "Epoch 190/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3360 - accuracy: 0.8493\n",
      "Epoch 190: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3306 - accuracy: 0.8497 - val_loss: 0.4869 - val_accuracy: 0.8436\n",
      "Epoch 191/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3433 - accuracy: 0.8399\n",
      "Epoch 191: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3433 - accuracy: 0.8399 - val_loss: 0.5007 - val_accuracy: 0.8324\n",
      "Epoch 192/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3395 - accuracy: 0.8409\n",
      "Epoch 192: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3393 - accuracy: 0.8413 - val_loss: 0.5027 - val_accuracy: 0.7933\n",
      "Epoch 193/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3549 - accuracy: 0.8467\n",
      "Epoch 193: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3549 - accuracy: 0.8497 - val_loss: 0.4976 - val_accuracy: 0.8268\n",
      "Epoch 194/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3376 - accuracy: 0.8409\n",
      "Epoch 194: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3392 - accuracy: 0.8413 - val_loss: 0.5096 - val_accuracy: 0.7933\n",
      "Epoch 195/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3239 - accuracy: 0.8537\n",
      "Epoch 195: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3261 - accuracy: 0.8511 - val_loss: 0.5097 - val_accuracy: 0.8380\n",
      "Epoch 196/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3426 - accuracy: 0.8364\n",
      "Epoch 196: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3421 - accuracy: 0.8385 - val_loss: 0.4897 - val_accuracy: 0.8212\n",
      "Epoch 197/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3641 - accuracy: 0.8467\n",
      "Epoch 197: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3628 - accuracy: 0.8441 - val_loss: 0.4889 - val_accuracy: 0.8436\n",
      "Epoch 198/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3349 - accuracy: 0.8494\n",
      "Epoch 198: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3372 - accuracy: 0.8483 - val_loss: 0.5096 - val_accuracy: 0.8156\n",
      "Epoch 199/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3471 - accuracy: 0.8452\n",
      "Epoch 199: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3452 - accuracy: 0.8469 - val_loss: 0.5245 - val_accuracy: 0.7877\n",
      "Epoch 200/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3308 - accuracy: 0.8531\n",
      "Epoch 200: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3343 - accuracy: 0.8525 - val_loss: 0.4975 - val_accuracy: 0.8380\n",
      "Epoch 201/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3414 - accuracy: 0.8421\n",
      "Epoch 201: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3458 - accuracy: 0.8413 - val_loss: 0.4999 - val_accuracy: 0.8436\n",
      "Epoch 202/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3321 - accuracy: 0.8602\n",
      "Epoch 202: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3423 - accuracy: 0.8553 - val_loss: 0.5029 - val_accuracy: 0.8324\n",
      "Epoch 203/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3338 - accuracy: 0.8557\n",
      "Epoch 203: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3324 - accuracy: 0.8581 - val_loss: 0.5124 - val_accuracy: 0.8156\n",
      "Epoch 204/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3331 - accuracy: 0.8484\n",
      "Epoch 204: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3297 - accuracy: 0.8511 - val_loss: 0.5182 - val_accuracy: 0.8268\n",
      "Epoch 205/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3377 - accuracy: 0.8355\n",
      "Epoch 205: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3329 - accuracy: 0.8357 - val_loss: 0.5110 - val_accuracy: 0.8101\n",
      "Epoch 206/500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.2393 - accuracy: 0.9062\n",
      "Epoch 206: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3245 - accuracy: 0.8511 - val_loss: 0.5139 - val_accuracy: 0.8212\n",
      "Epoch 207/500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.2303 - accuracy: 0.9062\n",
      "Epoch 207: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3354 - accuracy: 0.8539 - val_loss: 0.5170 - val_accuracy: 0.8380\n",
      "Epoch 208/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3416 - accuracy: 0.8493\n",
      "Epoch 208: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3295 - accuracy: 0.8567 - val_loss: 0.5091 - val_accuracy: 0.8045\n",
      "Epoch 209/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3128 - accuracy: 0.8641\n",
      "Epoch 209: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3239 - accuracy: 0.8624 - val_loss: 0.5287 - val_accuracy: 0.8268\n",
      "Epoch 210/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3314 - accuracy: 0.8524\n",
      "Epoch 210: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.3351 - accuracy: 0.8455 - val_loss: 0.5134 - val_accuracy: 0.8045\n",
      "Epoch 211/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3390 - accuracy: 0.8438\n",
      "Epoch 211: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3349 - accuracy: 0.8455 - val_loss: 0.5113 - val_accuracy: 0.8156\n",
      "Epoch 212/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3423 - accuracy: 0.8355\n",
      "Epoch 212: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.3286 - accuracy: 0.8413 - val_loss: 0.5207 - val_accuracy: 0.8045\n",
      "Epoch 213/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3306 - accuracy: 0.8423\n",
      "Epoch 213: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3312 - accuracy: 0.8413 - val_loss: 0.5182 - val_accuracy: 0.8156\n",
      "Epoch 214/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3320 - accuracy: 0.8594\n",
      "Epoch 214: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3376 - accuracy: 0.8581 - val_loss: 0.4979 - val_accuracy: 0.8268\n",
      "Epoch 215/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3393 - accuracy: 0.8378\n",
      "Epoch 215: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3422 - accuracy: 0.8385 - val_loss: 0.5269 - val_accuracy: 0.8268\n",
      "Epoch 216/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3217 - accuracy: 0.8409\n",
      "Epoch 216: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3286 - accuracy: 0.8371 - val_loss: 0.5256 - val_accuracy: 0.8436\n",
      "Epoch 217/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3277 - accuracy: 0.8469\n",
      "Epoch 217: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3298 - accuracy: 0.8455 - val_loss: 0.5226 - val_accuracy: 0.8212\n",
      "Epoch 218/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3227 - accuracy: 0.8503\n",
      "Epoch 218: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3310 - accuracy: 0.8497 - val_loss: 0.5359 - val_accuracy: 0.8324\n",
      "Epoch 219/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3422 - accuracy: 0.8456\n",
      "Epoch 219: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3311 - accuracy: 0.8553 - val_loss: 0.5220 - val_accuracy: 0.8101\n",
      "Epoch 220/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3285 - accuracy: 0.8507\n",
      "Epoch 220: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3244 - accuracy: 0.8525 - val_loss: 0.5428 - val_accuracy: 0.8156\n",
      "Epoch 221/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3168 - accuracy: 0.8520\n",
      "Epoch 221: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.3175 - accuracy: 0.8511 - val_loss: 0.5284 - val_accuracy: 0.8101\n",
      "Epoch 222/500\n",
      "15/23 [==================>...........] - ETA: 0s - loss: 0.3169 - accuracy: 0.8646\n",
      "Epoch 222: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3174 - accuracy: 0.8624 - val_loss: 0.5243 - val_accuracy: 0.8324\n",
      "Epoch 223/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3203 - accuracy: 0.8511\n",
      "Epoch 223: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3154 - accuracy: 0.8497 - val_loss: 0.5373 - val_accuracy: 0.8156\n",
      "Epoch 224/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3243 - accuracy: 0.8523\n",
      "Epoch 224: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3242 - accuracy: 0.8511 - val_loss: 0.5357 - val_accuracy: 0.8101\n",
      "Epoch 225/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3181 - accuracy: 0.8559\n",
      "Epoch 225: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.3256 - accuracy: 0.8539 - val_loss: 0.5454 - val_accuracy: 0.8212\n",
      "Epoch 226/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.3449 - accuracy: 0.8379\n",
      "Epoch 226: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3305 - accuracy: 0.8483 - val_loss: 0.5141 - val_accuracy: 0.8324\n",
      "Epoch 227/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3046 - accuracy: 0.8658\n",
      "Epoch 227: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3199 - accuracy: 0.8581 - val_loss: 0.5547 - val_accuracy: 0.8101\n",
      "Epoch 228/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3206 - accuracy: 0.8385\n",
      "Epoch 228: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.3341 - accuracy: 0.8343 - val_loss: 0.5380 - val_accuracy: 0.7933\n",
      "Epoch 229/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3345 - accuracy: 0.8403\n",
      "Epoch 229: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.3363 - accuracy: 0.8357 - val_loss: 0.4746 - val_accuracy: 0.8380\n",
      "Epoch 230/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3324 - accuracy: 0.8601\n",
      "Epoch 230: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3341 - accuracy: 0.8553 - val_loss: 0.4911 - val_accuracy: 0.8324\n",
      "Epoch 231/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.3101 - accuracy: 0.8691\n",
      "Epoch 231: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3302 - accuracy: 0.8539 - val_loss: 0.5235 - val_accuracy: 0.7989\n",
      "Epoch 232/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3355 - accuracy: 0.8327\n",
      "Epoch 232: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3440 - accuracy: 0.8385 - val_loss: 0.5234 - val_accuracy: 0.8380\n",
      "Epoch 233/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.3417 - accuracy: 0.8516\n",
      "Epoch 233: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3331 - accuracy: 0.8511 - val_loss: 0.5146 - val_accuracy: 0.8268\n",
      "Epoch 234/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3113 - accuracy: 0.8631\n",
      "Epoch 234: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3173 - accuracy: 0.8581 - val_loss: 0.5294 - val_accuracy: 0.7989\n",
      "Epoch 235/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.3348 - accuracy: 0.8418\n",
      "Epoch 235: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3224 - accuracy: 0.8511 - val_loss: 0.5185 - val_accuracy: 0.8324\n",
      "Epoch 236/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3099 - accuracy: 0.8631\n",
      "Epoch 236: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3139 - accuracy: 0.8652 - val_loss: 0.5151 - val_accuracy: 0.8212\n",
      "Epoch 237/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.3031 - accuracy: 0.8594\n",
      "Epoch 237: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3170 - accuracy: 0.8497 - val_loss: 0.5270 - val_accuracy: 0.8268\n",
      "Epoch 238/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3230 - accuracy: 0.8557\n",
      "Epoch 238: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3219 - accuracy: 0.8553 - val_loss: 0.5271 - val_accuracy: 0.8212\n",
      "Epoch 239/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3129 - accuracy: 0.8693\n",
      "Epoch 239: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.3142 - accuracy: 0.8666 - val_loss: 0.5147 - val_accuracy: 0.8380\n",
      "Epoch 240/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3323 - accuracy: 0.8601\n",
      "Epoch 240: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3429 - accuracy: 0.8539 - val_loss: 0.5375 - val_accuracy: 0.8101\n",
      "Epoch 241/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3187 - accuracy: 0.8484\n",
      "Epoch 241: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3197 - accuracy: 0.8483 - val_loss: 0.5316 - val_accuracy: 0.7933\n",
      "Epoch 242/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3199 - accuracy: 0.8497\n",
      "Epoch 242: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3231 - accuracy: 0.8469 - val_loss: 0.5147 - val_accuracy: 0.8324\n",
      "Epoch 243/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3233 - accuracy: 0.8601\n",
      "Epoch 243: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3228 - accuracy: 0.8624 - val_loss: 0.5314 - val_accuracy: 0.8268\n",
      "Epoch 244/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2890 - accuracy: 0.8783\n",
      "Epoch 244: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.3138 - accuracy: 0.8624 - val_loss: 0.5367 - val_accuracy: 0.8380\n",
      "Epoch 245/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3197 - accuracy: 0.8594\n",
      "Epoch 245: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.3163 - accuracy: 0.8610 - val_loss: 0.5233 - val_accuracy: 0.8212\n",
      "Epoch 246/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.3128 - accuracy: 0.8535\n",
      "Epoch 246: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3107 - accuracy: 0.8539 - val_loss: 0.5353 - val_accuracy: 0.8268\n",
      "Epoch 247/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3269 - accuracy: 0.8594\n",
      "Epoch 247: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3242 - accuracy: 0.8525 - val_loss: 0.5394 - val_accuracy: 0.8045\n",
      "Epoch 248/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3231 - accuracy: 0.8487\n",
      "Epoch 248: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.3256 - accuracy: 0.8469 - val_loss: 0.5229 - val_accuracy: 0.8324\n",
      "Epoch 249/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3279 - accuracy: 0.8651\n",
      "Epoch 249: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3261 - accuracy: 0.8666 - val_loss: 0.5439 - val_accuracy: 0.8156\n",
      "Epoch 250/500\n",
      "15/23 [==================>...........] - ETA: 0s - loss: 0.2908 - accuracy: 0.8625\n",
      "Epoch 250: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3155 - accuracy: 0.8455 - val_loss: 0.5516 - val_accuracy: 0.8101\n",
      "Epoch 251/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3166 - accuracy: 0.8566\n",
      "Epoch 251: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3230 - accuracy: 0.8596 - val_loss: 0.5156 - val_accuracy: 0.8045\n",
      "Epoch 252/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3239 - accuracy: 0.8511\n",
      "Epoch 252: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3153 - accuracy: 0.8581 - val_loss: 0.5439 - val_accuracy: 0.8268\n",
      "Epoch 253/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3085 - accuracy: 0.8687\n",
      "Epoch 253: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3159 - accuracy: 0.8652 - val_loss: 0.5440 - val_accuracy: 0.8268\n",
      "Epoch 254/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3154 - accuracy: 0.8531\n",
      "Epoch 254: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.3168 - accuracy: 0.8539 - val_loss: 0.5372 - val_accuracy: 0.8156\n",
      "Epoch 255/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3093 - accuracy: 0.8681\n",
      "Epoch 255: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.3075 - accuracy: 0.8596 - val_loss: 0.5467 - val_accuracy: 0.8380\n",
      "Epoch 256/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3167 - accuracy: 0.8542\n",
      "Epoch 256: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.3154 - accuracy: 0.8511 - val_loss: 0.5345 - val_accuracy: 0.8324\n",
      "Epoch 257/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3074 - accuracy: 0.8586\n",
      "Epoch 257: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3153 - accuracy: 0.8553 - val_loss: 0.5382 - val_accuracy: 0.8268\n",
      "Epoch 258/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3304 - accuracy: 0.8551\n",
      "Epoch 258: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3301 - accuracy: 0.8553 - val_loss: 0.5533 - val_accuracy: 0.7933\n",
      "Epoch 259/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3044 - accuracy: 0.8493\n",
      "Epoch 259: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3133 - accuracy: 0.8483 - val_loss: 0.5281 - val_accuracy: 0.8436\n",
      "Epoch 260/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3304 - accuracy: 0.8453\n",
      "Epoch 260: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3267 - accuracy: 0.8497 - val_loss: 0.5463 - val_accuracy: 0.8156\n",
      "Epoch 261/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.3153 - accuracy: 0.8535\n",
      "Epoch 261: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3013 - accuracy: 0.8610 - val_loss: 0.5733 - val_accuracy: 0.8156\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3182 - accuracy: 0.8539\n",
      "Epoch 262: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3182 - accuracy: 0.8539 - val_loss: 0.5294 - val_accuracy: 0.8380\n",
      "Epoch 263/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3097 - accuracy: 0.8576\n",
      "Epoch 263: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3117 - accuracy: 0.8596 - val_loss: 0.5712 - val_accuracy: 0.8101\n",
      "Epoch 264/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2937 - accuracy: 0.8703\n",
      "Epoch 264: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3030 - accuracy: 0.8638 - val_loss: 0.5462 - val_accuracy: 0.8212\n",
      "Epoch 265/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.2920 - accuracy: 0.8633\n",
      "Epoch 265: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3110 - accuracy: 0.8581 - val_loss: 0.5518 - val_accuracy: 0.7821\n",
      "Epoch 266/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2994 - accuracy: 0.8586\n",
      "Epoch 266: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.2992 - accuracy: 0.8581 - val_loss: 0.5294 - val_accuracy: 0.8045\n",
      "Epoch 267/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3075 - accuracy: 0.8701\n",
      "Epoch 267: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.3007 - accuracy: 0.8764 - val_loss: 0.5597 - val_accuracy: 0.8268\n",
      "Epoch 268/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3134 - accuracy: 0.8531\n",
      "Epoch 268: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3156 - accuracy: 0.8511 - val_loss: 0.5356 - val_accuracy: 0.7933\n",
      "Epoch 269/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.3045 - accuracy: 0.8750\n",
      "Epoch 269: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3075 - accuracy: 0.8708 - val_loss: 0.5420 - val_accuracy: 0.8324\n",
      "Epoch 270/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.2957 - accuracy: 0.8733\n",
      "Epoch 270: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.3086 - accuracy: 0.8596 - val_loss: 0.5474 - val_accuracy: 0.8268\n",
      "Epoch 271/500\n",
      "15/23 [==================>...........] - ETA: 0s - loss: 0.3272 - accuracy: 0.8625\n",
      "Epoch 271: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3067 - accuracy: 0.8694 - val_loss: 0.5375 - val_accuracy: 0.8156\n",
      "Epoch 272/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3019 - accuracy: 0.8781\n",
      "Epoch 272: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.3022 - accuracy: 0.8694 - val_loss: 0.5775 - val_accuracy: 0.8156\n",
      "Epoch 273/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3000 - accuracy: 0.8576\n",
      "Epoch 273: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.2996 - accuracy: 0.8581 - val_loss: 0.5499 - val_accuracy: 0.8380\n",
      "Epoch 274/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3109 - accuracy: 0.8537\n",
      "Epoch 274: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3097 - accuracy: 0.8539 - val_loss: 0.5751 - val_accuracy: 0.8268\n",
      "Epoch 275/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3050 - accuracy: 0.8548\n",
      "Epoch 275: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.3054 - accuracy: 0.8525 - val_loss: 0.5933 - val_accuracy: 0.7933\n",
      "Epoch 276/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3172 - accuracy: 0.8576\n",
      "Epoch 276: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3023 - accuracy: 0.8581 - val_loss: 0.5919 - val_accuracy: 0.8156\n",
      "Epoch 277/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3055 - accuracy: 0.8628\n",
      "Epoch 277: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.2989 - accuracy: 0.8666 - val_loss: 0.5762 - val_accuracy: 0.8268\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3167 - accuracy: 0.8539\n",
      "Epoch 278: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3167 - accuracy: 0.8539 - val_loss: 0.5715 - val_accuracy: 0.8268\n",
      "Epoch 279/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3089 - accuracy: 0.8631\n",
      "Epoch 279: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3125 - accuracy: 0.8624 - val_loss: 0.6049 - val_accuracy: 0.7933\n",
      "Epoch 280/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3058 - accuracy: 0.8750\n",
      "Epoch 280: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3062 - accuracy: 0.8750 - val_loss: 0.5732 - val_accuracy: 0.8268\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2989 - accuracy: 0.8652\n",
      "Epoch 281: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2989 - accuracy: 0.8652 - val_loss: 0.5750 - val_accuracy: 0.8212\n",
      "Epoch 282/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3133 - accuracy: 0.8616\n",
      "Epoch 282: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3241 - accuracy: 0.8624 - val_loss: 0.6186 - val_accuracy: 0.8045\n",
      "Epoch 283/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3198 - accuracy: 0.8676\n",
      "Epoch 283: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3264 - accuracy: 0.8539 - val_loss: 0.5551 - val_accuracy: 0.8380\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3065 - accuracy: 0.8581\n",
      "Epoch 284: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3065 - accuracy: 0.8581 - val_loss: 0.5821 - val_accuracy: 0.8212\n",
      "Epoch 285/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3028 - accuracy: 0.8732\n",
      "Epoch 285: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3046 - accuracy: 0.8652 - val_loss: 0.5896 - val_accuracy: 0.8156\n",
      "Epoch 286/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3153 - accuracy: 0.8527\n",
      "Epoch 286: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3154 - accuracy: 0.8497 - val_loss: 0.5861 - val_accuracy: 0.8156\n",
      "Epoch 287/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.3029 - accuracy: 0.8672\n",
      "Epoch 287: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3042 - accuracy: 0.8666 - val_loss: 0.5636 - val_accuracy: 0.8212\n",
      "Epoch 288/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3082 - accuracy: 0.8571\n",
      "Epoch 288: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3044 - accuracy: 0.8596 - val_loss: 0.5618 - val_accuracy: 0.8268\n",
      "Epoch 289/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3144 - accuracy: 0.8594\n",
      "Epoch 289: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3126 - accuracy: 0.8610 - val_loss: 0.5587 - val_accuracy: 0.8324\n",
      "Epoch 290/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.3077 - accuracy: 0.8562\n",
      "Epoch 290: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3009 - accuracy: 0.8610 - val_loss: 0.5861 - val_accuracy: 0.8324\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3005 - accuracy: 0.8624\n",
      "Epoch 291: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3005 - accuracy: 0.8624 - val_loss: 0.5685 - val_accuracy: 0.8156\n",
      "Epoch 292/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.2961 - accuracy: 0.8695\n",
      "Epoch 292: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2998 - accuracy: 0.8764 - val_loss: 0.5782 - val_accuracy: 0.8101\n",
      "Epoch 293/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3072 - accuracy: 0.8636\n",
      "Epoch 293: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3114 - accuracy: 0.8624 - val_loss: 0.5686 - val_accuracy: 0.8324\n",
      "Epoch 294/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3189 - accuracy: 0.8616\n",
      "Epoch 294: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3090 - accuracy: 0.8666 - val_loss: 0.5774 - val_accuracy: 0.8101\n",
      "Epoch 295/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3020 - accuracy: 0.8750\n",
      "Epoch 295: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3087 - accuracy: 0.8736 - val_loss: 0.6076 - val_accuracy: 0.8268\n",
      "Epoch 296/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3147 - accuracy: 0.8537\n",
      "Epoch 296: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3145 - accuracy: 0.8539 - val_loss: 0.5799 - val_accuracy: 0.8156\n",
      "Epoch 297/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3042 - accuracy: 0.8636\n",
      "Epoch 297: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3050 - accuracy: 0.8638 - val_loss: 0.5743 - val_accuracy: 0.8212\n",
      "Epoch 298/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2982 - accuracy: 0.8693\n",
      "Epoch 298: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2954 - accuracy: 0.8708 - val_loss: 0.6072 - val_accuracy: 0.8324\n",
      "Epoch 299/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3080 - accuracy: 0.8676\n",
      "Epoch 299: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3119 - accuracy: 0.8680 - val_loss: 0.5809 - val_accuracy: 0.8324\n",
      "Epoch 300/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.3007 - accuracy: 0.8602\n",
      "Epoch 300: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2963 - accuracy: 0.8638 - val_loss: 0.5858 - val_accuracy: 0.8101\n",
      "Epoch 301/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2911 - accuracy: 0.8722\n",
      "Epoch 301: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2951 - accuracy: 0.8722 - val_loss: 0.5969 - val_accuracy: 0.8101\n",
      "Epoch 302/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.3181 - accuracy: 0.8652\n",
      "Epoch 302: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3131 - accuracy: 0.8694 - val_loss: 0.6015 - val_accuracy: 0.7598\n",
      "Epoch 303/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3255 - accuracy: 0.8494\n",
      "Epoch 303: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3231 - accuracy: 0.8511 - val_loss: 0.6115 - val_accuracy: 0.7542\n",
      "Epoch 304/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.2985 - accuracy: 0.8555\n",
      "Epoch 304: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3083 - accuracy: 0.8525 - val_loss: 0.5779 - val_accuracy: 0.8380\n",
      "Epoch 305/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3142 - accuracy: 0.8640\n",
      "Epoch 305: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3178 - accuracy: 0.8652 - val_loss: 0.6087 - val_accuracy: 0.8268\n",
      "Epoch 306/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2919 - accuracy: 0.8693\n",
      "Epoch 306: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2953 - accuracy: 0.8666 - val_loss: 0.6155 - val_accuracy: 0.8324\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2964 - accuracy: 0.8778\n",
      "Epoch 307: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2964 - accuracy: 0.8778 - val_loss: 0.6029 - val_accuracy: 0.7989\n",
      "Epoch 308/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2868 - accuracy: 0.8618\n",
      "Epoch 308: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.3039 - accuracy: 0.8567 - val_loss: 0.6249 - val_accuracy: 0.8324\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2958 - accuracy: 0.8680\n",
      "Epoch 309: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2958 - accuracy: 0.8680 - val_loss: 0.5720 - val_accuracy: 0.8045\n",
      "Epoch 310/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3139 - accuracy: 0.8566\n",
      "Epoch 310: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.2944 - accuracy: 0.8652 - val_loss: 0.6113 - val_accuracy: 0.7989\n",
      "Epoch 311/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3016 - accuracy: 0.8571\n",
      "Epoch 311: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3102 - accuracy: 0.8567 - val_loss: 0.5957 - val_accuracy: 0.8101\n",
      "Epoch 312/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.3063 - accuracy: 0.8601\n",
      "Epoch 312: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.3032 - accuracy: 0.8652 - val_loss: 0.5578 - val_accuracy: 0.8212\n",
      "Epoch 313/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2902 - accuracy: 0.8580\n",
      "Epoch 313: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.2893 - accuracy: 0.8596 - val_loss: 0.5802 - val_accuracy: 0.8212\n",
      "Epoch 314/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2956 - accuracy: 0.8764\n",
      "Epoch 314: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2945 - accuracy: 0.8764 - val_loss: 0.5795 - val_accuracy: 0.7933\n",
      "Epoch 315/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2892 - accuracy: 0.8891\n",
      "Epoch 315: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2852 - accuracy: 0.8890 - val_loss: 0.6031 - val_accuracy: 0.8212\n",
      "Epoch 316/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2770 - accuracy: 0.8719\n",
      "Epoch 316: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2968 - accuracy: 0.8652 - val_loss: 0.5859 - val_accuracy: 0.8212\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.8750\n",
      "Epoch 317: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2858 - accuracy: 0.8750 - val_loss: 0.5935 - val_accuracy: 0.8101\n",
      "Epoch 318/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2892 - accuracy: 0.8736\n",
      "Epoch 318: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2893 - accuracy: 0.8722 - val_loss: 0.6228 - val_accuracy: 0.8268\n",
      "Epoch 319/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2961 - accuracy: 0.8750\n",
      "Epoch 319: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2956 - accuracy: 0.8750 - val_loss: 0.5962 - val_accuracy: 0.8156\n",
      "Epoch 320/500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.1320 - accuracy: 0.9688\n",
      "Epoch 320: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2969 - accuracy: 0.8764 - val_loss: 0.6131 - val_accuracy: 0.8212\n",
      "Epoch 321/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2923 - accuracy: 0.8717\n",
      "Epoch 321: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2904 - accuracy: 0.8764 - val_loss: 0.6074 - val_accuracy: 0.8156\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2803 - accuracy: 0.8764\n",
      "Epoch 322: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2803 - accuracy: 0.8764 - val_loss: 0.6256 - val_accuracy: 0.8268\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.3026 - accuracy: 0.8666\n",
      "Epoch 323: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3026 - accuracy: 0.8666 - val_loss: 0.6039 - val_accuracy: 0.8045\n",
      "Epoch 324/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3026 - accuracy: 0.8733\n",
      "Epoch 324: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3122 - accuracy: 0.8680 - val_loss: 0.6087 - val_accuracy: 0.8101\n",
      "Epoch 325/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2939 - accuracy: 0.8571\n",
      "Epoch 325: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2974 - accuracy: 0.8581 - val_loss: 0.6200 - val_accuracy: 0.8101\n",
      "Epoch 326/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2798 - accuracy: 0.8750\n",
      "Epoch 326: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2828 - accuracy: 0.8736 - val_loss: 0.6079 - val_accuracy: 0.7989\n",
      "Epoch 327/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.3175 - accuracy: 0.8566\n",
      "Epoch 327: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2997 - accuracy: 0.8666 - val_loss: 0.5808 - val_accuracy: 0.8045\n",
      "Epoch 328/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.2998 - accuracy: 0.8785\n",
      "Epoch 328: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2985 - accuracy: 0.8778 - val_loss: 0.6368 - val_accuracy: 0.8156\n",
      "Epoch 329/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.2772 - accuracy: 0.8854\n",
      "Epoch 329: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2916 - accuracy: 0.8792 - val_loss: 0.6258 - val_accuracy: 0.7877\n",
      "Epoch 330/500\n",
      " 9/23 [==========>...................] - ETA: 0s - loss: 0.2274 - accuracy: 0.9201\n",
      "Epoch 330: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2969 - accuracy: 0.8722 - val_loss: 0.6543 - val_accuracy: 0.7877\n",
      "Epoch 331/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.2885 - accuracy: 0.8750\n",
      "Epoch 331: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.3096 - accuracy: 0.8624 - val_loss: 0.6181 - val_accuracy: 0.8212\n",
      "Epoch 332/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2790 - accuracy: 0.8828\n",
      "Epoch 332: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2898 - accuracy: 0.8778 - val_loss: 0.6254 - val_accuracy: 0.8212\n",
      "Epoch 333/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2967 - accuracy: 0.8750\n",
      "Epoch 333: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2977 - accuracy: 0.8736 - val_loss: 0.6032 - val_accuracy: 0.8101\n",
      "Epoch 334/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2821 - accuracy: 0.8766\n",
      "Epoch 334: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2939 - accuracy: 0.8708 - val_loss: 0.6278 - val_accuracy: 0.8156\n",
      "Epoch 335/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.2959 - accuracy: 0.8715\n",
      "Epoch 335: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2943 - accuracy: 0.8722 - val_loss: 0.6107 - val_accuracy: 0.8156\n",
      "Epoch 336/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3009 - accuracy: 0.8698\n",
      "Epoch 336: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2904 - accuracy: 0.8764 - val_loss: 0.6172 - val_accuracy: 0.7877\n",
      "Epoch 337/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2900 - accuracy: 0.8672\n",
      "Epoch 337: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2981 - accuracy: 0.8624 - val_loss: 0.6088 - val_accuracy: 0.8045\n",
      "Epoch 338/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.2958 - accuracy: 0.8787\n",
      "Epoch 338: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2816 - accuracy: 0.8848 - val_loss: 0.5789 - val_accuracy: 0.8212\n",
      "Epoch 339/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2784 - accuracy: 0.8734\n",
      "Epoch 339: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2919 - accuracy: 0.8680 - val_loss: 0.5987 - val_accuracy: 0.8101\n",
      "Epoch 340/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2865 - accuracy: 0.8832\n",
      "Epoch 340: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2814 - accuracy: 0.8862 - val_loss: 0.6054 - val_accuracy: 0.8212\n",
      "Epoch 341/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2906 - accuracy: 0.8778\n",
      "Epoch 341: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2902 - accuracy: 0.8778 - val_loss: 0.5995 - val_accuracy: 0.8212\n",
      "Epoch 342/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2799 - accuracy: 0.8735\n",
      "Epoch 342: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2790 - accuracy: 0.8736 - val_loss: 0.6153 - val_accuracy: 0.8212\n",
      "Epoch 343/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2834 - accuracy: 0.8766\n",
      "Epoch 343: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2889 - accuracy: 0.8722 - val_loss: 0.6269 - val_accuracy: 0.8045\n",
      "Epoch 344/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2719 - accuracy: 0.8766\n",
      "Epoch 344: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2885 - accuracy: 0.8680 - val_loss: 0.6187 - val_accuracy: 0.8268\n",
      "Epoch 345/500\n",
      " 7/23 [========>.....................] - ETA: 0s - loss: 0.3174 - accuracy: 0.8616\n",
      "Epoch 345: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2871 - accuracy: 0.8694 - val_loss: 0.6071 - val_accuracy: 0.8268\n",
      "Epoch 346/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2828 - accuracy: 0.8795\n",
      "Epoch 346: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2886 - accuracy: 0.8778 - val_loss: 0.6472 - val_accuracy: 0.8101\n",
      "Epoch 347/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2869 - accuracy: 0.8797\n",
      "Epoch 347: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2889 - accuracy: 0.8778 - val_loss: 0.6147 - val_accuracy: 0.8045\n",
      "Epoch 348/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2892 - accuracy: 0.8705\n",
      "Epoch 348: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2839 - accuracy: 0.8736 - val_loss: 0.6237 - val_accuracy: 0.8324\n",
      "Epoch 349/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.2786 - accuracy: 0.8805\n",
      "Epoch 349: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2752 - accuracy: 0.8750 - val_loss: 0.6662 - val_accuracy: 0.7989\n",
      "Epoch 350/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2918 - accuracy: 0.8735\n",
      "Epoch 350: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2889 - accuracy: 0.8764 - val_loss: 0.6391 - val_accuracy: 0.8101\n",
      "Epoch 351/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2750 - accuracy: 0.8780\n",
      "Epoch 351: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2771 - accuracy: 0.8750 - val_loss: 0.6341 - val_accuracy: 0.8156\n",
      "Epoch 352/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2802 - accuracy: 0.8765\n",
      "Epoch 352: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.2815 - accuracy: 0.8750 - val_loss: 0.6739 - val_accuracy: 0.8212\n",
      "Epoch 353/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2884 - accuracy: 0.8734\n",
      "Epoch 353: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2929 - accuracy: 0.8736 - val_loss: 0.6373 - val_accuracy: 0.7933\n",
      "Epoch 354/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.2716 - accuracy: 0.8848\n",
      "Epoch 354: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2824 - accuracy: 0.8722 - val_loss: 0.6641 - val_accuracy: 0.7933\n",
      "Epoch 355/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2703 - accuracy: 0.8816\n",
      "Epoch 355: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.2708 - accuracy: 0.8820 - val_loss: 0.6510 - val_accuracy: 0.8101\n",
      "Epoch 356/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.2755 - accuracy: 0.8713\n",
      "Epoch 356: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.2802 - accuracy: 0.8680 - val_loss: 0.6441 - val_accuracy: 0.8101\n",
      "Epoch 357/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2773 - accuracy: 0.8854\n",
      "Epoch 357: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2781 - accuracy: 0.8848 - val_loss: 0.6173 - val_accuracy: 0.8156\n",
      "Epoch 358/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2699 - accuracy: 0.8828\n",
      "Epoch 358: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2844 - accuracy: 0.8736 - val_loss: 0.6120 - val_accuracy: 0.8156\n",
      "Epoch 359/500\n",
      "11/23 [=============>................] - ETA: 0s - loss: 0.2419 - accuracy: 0.9006\n",
      "Epoch 359: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2884 - accuracy: 0.8736 - val_loss: 0.5840 - val_accuracy: 0.8101\n",
      "Epoch 360/500\n",
      "14/23 [=================>............] - ETA: 0s - loss: 0.2981 - accuracy: 0.8549\n",
      "Epoch 360: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3001 - accuracy: 0.8638 - val_loss: 0.6186 - val_accuracy: 0.7989\n",
      "Epoch 361/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2805 - accuracy: 0.8750\n",
      "Epoch 361: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2899 - accuracy: 0.8680 - val_loss: 0.6520 - val_accuracy: 0.8101\n",
      "Epoch 362/500\n",
      "14/23 [=================>............] - ETA: 0s - loss: 0.2692 - accuracy: 0.8705\n",
      "Epoch 362: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2809 - accuracy: 0.8666 - val_loss: 0.6103 - val_accuracy: 0.8324\n",
      "Epoch 363/500\n",
      "10/23 [============>.................] - ETA: 0s - loss: 0.2795 - accuracy: 0.8719\n",
      "Epoch 363: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2841 - accuracy: 0.8722 - val_loss: 0.6142 - val_accuracy: 0.8156\n",
      "Epoch 364/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2755 - accuracy: 0.8736\n",
      "Epoch 364: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2736 - accuracy: 0.8750 - val_loss: 0.6865 - val_accuracy: 0.8101\n",
      "Epoch 365/500\n",
      "15/23 [==================>...........] - ETA: 0s - loss: 0.2803 - accuracy: 0.8708\n",
      "Epoch 365: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2709 - accuracy: 0.8778 - val_loss: 0.6679 - val_accuracy: 0.8101\n",
      "Epoch 366/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.2765 - accuracy: 0.8906\n",
      "Epoch 366: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2686 - accuracy: 0.8890 - val_loss: 0.6381 - val_accuracy: 0.8101\n",
      "Epoch 367/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.2936 - accuracy: 0.8767\n",
      "Epoch 367: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2826 - accuracy: 0.8862 - val_loss: 0.6374 - val_accuracy: 0.8324\n",
      "Epoch 368/500\n",
      "13/23 [===============>..............] - ETA: 0s - loss: 0.2510 - accuracy: 0.8942\n",
      "Epoch 368: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2652 - accuracy: 0.8848 - val_loss: 0.6607 - val_accuracy: 0.8212\n",
      "Epoch 369/500\n",
      "12/23 [==============>...............] - ETA: 0s - loss: 0.2937 - accuracy: 0.8776\n",
      "Epoch 369: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2709 - accuracy: 0.8876 - val_loss: 0.6218 - val_accuracy: 0.8212\n",
      "Epoch 370/500\n",
      "14/23 [=================>............] - ETA: 0s - loss: 0.2408 - accuracy: 0.8929\n",
      "Epoch 370: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2780 - accuracy: 0.8722 - val_loss: 0.7018 - val_accuracy: 0.8101\n",
      "Epoch 371/500\n",
      "12/23 [==============>...............] - ETA: 0s - loss: 0.2874 - accuracy: 0.8854\n",
      "Epoch 371: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2847 - accuracy: 0.8834 - val_loss: 0.6206 - val_accuracy: 0.8324\n",
      "Epoch 372/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.2822 - accuracy: 0.8672\n",
      "Epoch 372: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2862 - accuracy: 0.8722 - val_loss: 0.6822 - val_accuracy: 0.8101\n",
      "Epoch 373/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2685 - accuracy: 0.8821\n",
      "Epoch 373: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2728 - accuracy: 0.8820 - val_loss: 0.6726 - val_accuracy: 0.7989\n",
      "Epoch 374/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.2938 - accuracy: 0.8805\n",
      "Epoch 374: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2871 - accuracy: 0.8848 - val_loss: 0.6489 - val_accuracy: 0.7989\n",
      "Epoch 375/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2832 - accuracy: 0.8844\n",
      "Epoch 375: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2713 - accuracy: 0.8890 - val_loss: 0.6762 - val_accuracy: 0.8045\n",
      "Epoch 376/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2774 - accuracy: 0.8764\n",
      "Epoch 376: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2805 - accuracy: 0.8750 - val_loss: 0.6346 - val_accuracy: 0.7989\n",
      "Epoch 377/500\n",
      "10/23 [============>.................] - ETA: 0s - loss: 0.2471 - accuracy: 0.8750\n",
      "Epoch 377: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2718 - accuracy: 0.8722 - val_loss: 0.6512 - val_accuracy: 0.8045\n",
      "Epoch 378/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2680 - accuracy: 0.8719\n",
      "Epoch 378: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.2695 - accuracy: 0.8722 - val_loss: 0.6501 - val_accuracy: 0.7989\n",
      "Epoch 379/500\n",
      "12/23 [==============>...............] - ETA: 0s - loss: 0.2976 - accuracy: 0.8646\n",
      "Epoch 379: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2738 - accuracy: 0.8764 - val_loss: 0.7298 - val_accuracy: 0.7933\n",
      "Epoch 380/500\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.2731 - accuracy: 0.8867\n",
      "Epoch 380: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.2850 - accuracy: 0.8820 - val_loss: 0.6331 - val_accuracy: 0.8212\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2736 - accuracy: 0.8778\n",
      "Epoch 381: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2736 - accuracy: 0.8778 - val_loss: 0.6525 - val_accuracy: 0.8045\n",
      "Epoch 382/500\n",
      "13/23 [===============>..............] - ETA: 0s - loss: 0.2572 - accuracy: 0.8870\n",
      "Epoch 382: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2598 - accuracy: 0.8792 - val_loss: 0.6364 - val_accuracy: 0.8156\n",
      "Epoch 383/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2580 - accuracy: 0.8797\n",
      "Epoch 383: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 0.2791 - accuracy: 0.8764 - val_loss: 0.6758 - val_accuracy: 0.7877\n",
      "Epoch 384/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.2826 - accuracy: 0.8767\n",
      "Epoch 384: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2843 - accuracy: 0.8736 - val_loss: 0.6600 - val_accuracy: 0.8156\n",
      "Epoch 385/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.2737 - accuracy: 0.8854\n",
      "Epoch 385: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.2759 - accuracy: 0.8862 - val_loss: 0.6487 - val_accuracy: 0.8045\n",
      "Epoch 386/500\n",
      "14/23 [=================>............] - ETA: 0s - loss: 0.2812 - accuracy: 0.8661\n",
      "Epoch 386: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.2661 - accuracy: 0.8806 - val_loss: 0.6873 - val_accuracy: 0.8045\n",
      "Epoch 387/500\n",
      "13/23 [===============>..............] - ETA: 0s - loss: 0.2715 - accuracy: 0.8966\n",
      "Epoch 387: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.2704 - accuracy: 0.8989 - val_loss: 0.6356 - val_accuracy: 0.8212\n",
      "Epoch 388/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2695 - accuracy: 0.8734\n",
      "Epoch 388: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2670 - accuracy: 0.8792 - val_loss: 0.6483 - val_accuracy: 0.8156\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2736 - accuracy: 0.8736\n",
      "Epoch 389: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.2736 - accuracy: 0.8736 - val_loss: 0.6728 - val_accuracy: 0.8212\n",
      "Epoch 390/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2772 - accuracy: 0.8734\n",
      "Epoch 390: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.2712 - accuracy: 0.8722 - val_loss: 0.6412 - val_accuracy: 0.8324\n",
      "Epoch 391/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.2687 - accuracy: 0.8879\n",
      "Epoch 391: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.2778 - accuracy: 0.8820 - val_loss: 0.6428 - val_accuracy: 0.8212\n",
      "Epoch 392/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.2720 - accuracy: 0.8879\n",
      "Epoch 392: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.2721 - accuracy: 0.8806 - val_loss: 0.6513 - val_accuracy: 0.8156\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2738 - accuracy: 0.8806\n",
      "Epoch 393: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2738 - accuracy: 0.8806 - val_loss: 0.7039 - val_accuracy: 0.8045\n",
      "Epoch 394/500\n",
      "14/23 [=================>............] - ETA: 0s - loss: 0.2752 - accuracy: 0.8839\n",
      "Epoch 394: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2856 - accuracy: 0.8834 - val_loss: 0.6726 - val_accuracy: 0.7989\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2676 - accuracy: 0.8890\n",
      "Epoch 395: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2676 - accuracy: 0.8890 - val_loss: 0.6781 - val_accuracy: 0.7933\n",
      "Epoch 396/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2750 - accuracy: 0.8799\n",
      "Epoch 396: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2763 - accuracy: 0.8764 - val_loss: 0.6683 - val_accuracy: 0.8268\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2624 - accuracy: 0.8989\n",
      "Epoch 397: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2624 - accuracy: 0.8989 - val_loss: 0.6648 - val_accuracy: 0.8156\n",
      "Epoch 398/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2519 - accuracy: 0.8906\n",
      "Epoch 398: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2584 - accuracy: 0.8792 - val_loss: 0.6676 - val_accuracy: 0.8212\n",
      "Epoch 399/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2727 - accuracy: 0.8821\n",
      "Epoch 399: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2769 - accuracy: 0.8792 - val_loss: 0.6496 - val_accuracy: 0.8101\n",
      "Epoch 400/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2717 - accuracy: 0.8750\n",
      "Epoch 400: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2721 - accuracy: 0.8750 - val_loss: 0.6876 - val_accuracy: 0.8045\n",
      "Epoch 401/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2708 - accuracy: 0.8795\n",
      "Epoch 401: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2694 - accuracy: 0.8778 - val_loss: 0.7475 - val_accuracy: 0.8101\n",
      "Epoch 402/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2803 - accuracy: 0.8807\n",
      "Epoch 402: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.2781 - accuracy: 0.8820 - val_loss: 0.6994 - val_accuracy: 0.7989\n",
      "Epoch 403/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.3253 - accuracy: 0.8819\n",
      "Epoch 403: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3122 - accuracy: 0.8806 - val_loss: 0.6499 - val_accuracy: 0.8156\n",
      "Epoch 404/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2759 - accuracy: 0.8750\n",
      "Epoch 404: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.2712 - accuracy: 0.8806 - val_loss: 0.6597 - val_accuracy: 0.8101\n",
      "Epoch 405/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2759 - accuracy: 0.8781\n",
      "Epoch 405: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2664 - accuracy: 0.8820 - val_loss: 0.6585 - val_accuracy: 0.8212\n",
      "Epoch 406/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2614 - accuracy: 0.8832\n",
      "Epoch 406: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2719 - accuracy: 0.8778 - val_loss: 0.6710 - val_accuracy: 0.8156\n",
      "Epoch 407/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2517 - accuracy: 0.8988\n",
      "Epoch 407: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2559 - accuracy: 0.8961 - val_loss: 0.6565 - val_accuracy: 0.8212\n",
      "Epoch 408/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2714 - accuracy: 0.8765\n",
      "Epoch 408: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.2654 - accuracy: 0.8792 - val_loss: 0.6949 - val_accuracy: 0.8045\n",
      "Epoch 409/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2470 - accuracy: 0.8898\n",
      "Epoch 409: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2534 - accuracy: 0.8904 - val_loss: 0.7018 - val_accuracy: 0.8212\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.8890\n",
      "Epoch 410: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2577 - accuracy: 0.8890 - val_loss: 0.6606 - val_accuracy: 0.8156\n",
      "Epoch 411/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2746 - accuracy: 0.8839\n",
      "Epoch 411: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2773 - accuracy: 0.8806 - val_loss: 0.6736 - val_accuracy: 0.8045\n",
      "Epoch 412/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2520 - accuracy: 0.8839\n",
      "Epoch 412: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2522 - accuracy: 0.8834 - val_loss: 0.6654 - val_accuracy: 0.7989\n",
      "Epoch 413/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2712 - accuracy: 0.8781\n",
      "Epoch 413: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2663 - accuracy: 0.8778 - val_loss: 0.7171 - val_accuracy: 0.7989\n",
      "Epoch 414/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.2465 - accuracy: 0.8879\n",
      "Epoch 414: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2574 - accuracy: 0.8890 - val_loss: 0.7117 - val_accuracy: 0.8156\n",
      "Epoch 415/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.2444 - accuracy: 0.9045\n",
      "Epoch 415: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2587 - accuracy: 0.8989 - val_loss: 0.7387 - val_accuracy: 0.7877\n",
      "Epoch 416/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2455 - accuracy: 0.8914\n",
      "Epoch 416: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2563 - accuracy: 0.8862 - val_loss: 0.7040 - val_accuracy: 0.8101\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2565 - accuracy: 0.8919\n",
      "Epoch 417: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2565 - accuracy: 0.8919 - val_loss: 0.7207 - val_accuracy: 0.8156\n",
      "Epoch 418/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2717 - accuracy: 0.8899\n",
      "Epoch 418: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2705 - accuracy: 0.8890 - val_loss: 0.7396 - val_accuracy: 0.8045\n",
      "Epoch 419/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2538 - accuracy: 0.8835\n",
      "Epoch 419: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2535 - accuracy: 0.8834 - val_loss: 0.7361 - val_accuracy: 0.8101\n",
      "Epoch 420/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2749 - accuracy: 0.8766\n",
      "Epoch 420: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2636 - accuracy: 0.8820 - val_loss: 0.7401 - val_accuracy: 0.8101\n",
      "Epoch 421/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2493 - accuracy: 0.9000\n",
      "Epoch 421: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2467 - accuracy: 0.8989 - val_loss: 0.7129 - val_accuracy: 0.8045\n",
      "Epoch 422/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2536 - accuracy: 0.9048\n",
      "Epoch 422: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2606 - accuracy: 0.9003 - val_loss: 0.7480 - val_accuracy: 0.8212\n",
      "Epoch 423/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.3040 - accuracy: 0.8665\n",
      "Epoch 423: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3085 - accuracy: 0.8666 - val_loss: 0.7628 - val_accuracy: 0.7709\n",
      "Epoch 424/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2870 - accuracy: 0.8750\n",
      "Epoch 424: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2855 - accuracy: 0.8750 - val_loss: 0.6601 - val_accuracy: 0.7821\n",
      "Epoch 425/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2805 - accuracy: 0.8734\n",
      "Epoch 425: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2757 - accuracy: 0.8736 - val_loss: 0.6828 - val_accuracy: 0.8156\n",
      "Epoch 426/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2791 - accuracy: 0.8795\n",
      "Epoch 426: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2825 - accuracy: 0.8778 - val_loss: 0.7280 - val_accuracy: 0.7989\n",
      "Epoch 427/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2609 - accuracy: 0.8914\n",
      "Epoch 427: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2607 - accuracy: 0.8904 - val_loss: 0.6776 - val_accuracy: 0.8156\n",
      "Epoch 428/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2779 - accuracy: 0.8844\n",
      "Epoch 428: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2712 - accuracy: 0.8862 - val_loss: 0.6817 - val_accuracy: 0.8101\n",
      "Epoch 429/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2577 - accuracy: 0.8969\n",
      "Epoch 429: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2688 - accuracy: 0.8904 - val_loss: 0.6886 - val_accuracy: 0.7877\n",
      "Epoch 430/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2693 - accuracy: 0.8906\n",
      "Epoch 430: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2689 - accuracy: 0.8876 - val_loss: 0.6910 - val_accuracy: 0.8101\n",
      "Epoch 431/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2708 - accuracy: 0.8839\n",
      "Epoch 431: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2686 - accuracy: 0.8862 - val_loss: 0.6672 - val_accuracy: 0.8101\n",
      "Epoch 432/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2701 - accuracy: 0.8781\n",
      "Epoch 432: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2683 - accuracy: 0.8792 - val_loss: 0.6669 - val_accuracy: 0.8156\n",
      "Epoch 433/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2509 - accuracy: 0.8938\n",
      "Epoch 433: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2488 - accuracy: 0.8933 - val_loss: 0.6620 - val_accuracy: 0.8045\n",
      "Epoch 434/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2775 - accuracy: 0.8844\n",
      "Epoch 434: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2746 - accuracy: 0.8792 - val_loss: 0.6901 - val_accuracy: 0.8268\n",
      "Epoch 435/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2756 - accuracy: 0.8899\n",
      "Epoch 435: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2746 - accuracy: 0.8876 - val_loss: 0.6153 - val_accuracy: 0.8212\n",
      "Epoch 436/500\n",
      "17/23 [=====================>........] - ETA: 0s - loss: 0.2662 - accuracy: 0.8860\n",
      "Epoch 436: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2704 - accuracy: 0.8862 - val_loss: 0.7484 - val_accuracy: 0.7989\n",
      "Epoch 437/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2835 - accuracy: 0.8795\n",
      "Epoch 437: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2794 - accuracy: 0.8820 - val_loss: 0.6387 - val_accuracy: 0.8101\n",
      "Epoch 438/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2825 - accuracy: 0.8701\n",
      "Epoch 438: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2745 - accuracy: 0.8722 - val_loss: 0.6542 - val_accuracy: 0.8212\n",
      "Epoch 439/500\n",
      "18/23 [======================>.......] - ETA: 0s - loss: 0.2527 - accuracy: 0.8785\n",
      "Epoch 439: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 0.2630 - accuracy: 0.8792 - val_loss: 0.6303 - val_accuracy: 0.7933\n",
      "Epoch 440/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2479 - accuracy: 0.9003\n",
      "Epoch 440: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2516 - accuracy: 0.8975 - val_loss: 0.7255 - val_accuracy: 0.8156\n",
      "Epoch 441/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2527 - accuracy: 0.8878\n",
      "Epoch 441: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2529 - accuracy: 0.8876 - val_loss: 0.6730 - val_accuracy: 0.8324\n",
      "Epoch 442/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2588 - accuracy: 0.8906\n",
      "Epoch 442: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2572 - accuracy: 0.8919 - val_loss: 0.6565 - val_accuracy: 0.8156\n",
      "Epoch 443/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2617 - accuracy: 0.8878\n",
      "Epoch 443: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2591 - accuracy: 0.8890 - val_loss: 0.6940 - val_accuracy: 0.8101\n",
      "Epoch 444/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2695 - accuracy: 0.8878\n",
      "Epoch 444: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2679 - accuracy: 0.8890 - val_loss: 0.6708 - val_accuracy: 0.8101\n",
      "Epoch 445/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2746 - accuracy: 0.8816\n",
      "Epoch 445: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2674 - accuracy: 0.8848 - val_loss: 0.6819 - val_accuracy: 0.8156\n",
      "Epoch 446/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2773 - accuracy: 0.8765\n",
      "Epoch 446: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2702 - accuracy: 0.8792 - val_loss: 0.6976 - val_accuracy: 0.8156\n",
      "Epoch 447/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2558 - accuracy: 0.8931\n",
      "Epoch 447: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2657 - accuracy: 0.8890 - val_loss: 0.7157 - val_accuracy: 0.8156\n",
      "Epoch 448/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2747 - accuracy: 0.8899\n",
      "Epoch 448: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2680 - accuracy: 0.8933 - val_loss: 0.6624 - val_accuracy: 0.8268\n",
      "Epoch 449/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2569 - accuracy: 0.8835\n",
      "Epoch 449: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2577 - accuracy: 0.8820 - val_loss: 0.6746 - val_accuracy: 0.8101\n",
      "Epoch 450/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2445 - accuracy: 0.8988\n",
      "Epoch 450: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2532 - accuracy: 0.8947 - val_loss: 0.7009 - val_accuracy: 0.8101\n",
      "Epoch 451/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2499 - accuracy: 0.8864\n",
      "Epoch 451: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2523 - accuracy: 0.8848 - val_loss: 0.7365 - val_accuracy: 0.7989\n",
      "Epoch 452/500\n",
      "15/23 [==================>...........] - ETA: 0s - loss: 0.2707 - accuracy: 0.8875\n",
      "Epoch 452: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2597 - accuracy: 0.8933 - val_loss: 0.6753 - val_accuracy: 0.8045\n",
      "Epoch 453/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2677 - accuracy: 0.8781\n",
      "Epoch 453: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2665 - accuracy: 0.8778 - val_loss: 0.7207 - val_accuracy: 0.8156\n",
      "Epoch 454/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2724 - accuracy: 0.8869\n",
      "Epoch 454: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2767 - accuracy: 0.8820 - val_loss: 0.6501 - val_accuracy: 0.7989\n",
      "Epoch 455/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2653 - accuracy: 0.8793\n",
      "Epoch 455: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2661 - accuracy: 0.8792 - val_loss: 0.6506 - val_accuracy: 0.8045\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - ETA: 0s - loss: 0.2556 - accuracy: 0.8862\n",
      "Epoch 456: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.2556 - accuracy: 0.8862 - val_loss: 0.6373 - val_accuracy: 0.8101\n",
      "Epoch 457/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2537 - accuracy: 0.8906\n",
      "Epoch 457: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2533 - accuracy: 0.8876 - val_loss: 0.7029 - val_accuracy: 0.7709\n",
      "Epoch 458/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2582 - accuracy: 0.8949\n",
      "Epoch 458: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2582 - accuracy: 0.8947 - val_loss: 0.6764 - val_accuracy: 0.8045\n",
      "Epoch 459/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2678 - accuracy: 0.8828\n",
      "Epoch 459: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2633 - accuracy: 0.8834 - val_loss: 0.7093 - val_accuracy: 0.8101\n",
      "Epoch 460/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2572 - accuracy: 0.8864\n",
      "Epoch 460: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2546 - accuracy: 0.8876 - val_loss: 0.6742 - val_accuracy: 0.8156\n",
      "Epoch 461/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2572 - accuracy: 0.8891\n",
      "Epoch 461: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2599 - accuracy: 0.8904 - val_loss: 0.7299 - val_accuracy: 0.8101\n",
      "Epoch 462/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2503 - accuracy: 0.8844\n",
      "Epoch 462: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2529 - accuracy: 0.8890 - val_loss: 0.7192 - val_accuracy: 0.8045\n",
      "Epoch 463/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2440 - accuracy: 0.8953\n",
      "Epoch 463: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2425 - accuracy: 0.8947 - val_loss: 0.6995 - val_accuracy: 0.8045\n",
      "Epoch 464/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2411 - accuracy: 0.9077\n",
      "Epoch 464: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2490 - accuracy: 0.8961 - val_loss: 0.7222 - val_accuracy: 0.8101\n",
      "Epoch 465/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2512 - accuracy: 0.8892\n",
      "Epoch 465: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2499 - accuracy: 0.8904 - val_loss: 0.7089 - val_accuracy: 0.8101\n",
      "Epoch 466/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2340 - accuracy: 0.8969\n",
      "Epoch 466: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2533 - accuracy: 0.8904 - val_loss: 0.7526 - val_accuracy: 0.8101\n",
      "Epoch 467/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2508 - accuracy: 0.9003\n",
      "Epoch 467: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2584 - accuracy: 0.8961 - val_loss: 0.6834 - val_accuracy: 0.8101\n",
      "Epoch 468/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2723 - accuracy: 0.8828\n",
      "Epoch 468: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 16ms/step - loss: 0.2679 - accuracy: 0.8862 - val_loss: 0.6778 - val_accuracy: 0.7989\n",
      "Epoch 469/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2704 - accuracy: 0.8882\n",
      "Epoch 469: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2669 - accuracy: 0.8890 - val_loss: 0.6587 - val_accuracy: 0.8101\n",
      "Epoch 470/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2657 - accuracy: 0.8764\n",
      "Epoch 470: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2646 - accuracy: 0.8778 - val_loss: 0.6606 - val_accuracy: 0.8101\n",
      "Epoch 471/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2464 - accuracy: 0.8969\n",
      "Epoch 471: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2585 - accuracy: 0.8862 - val_loss: 0.7337 - val_accuracy: 0.7989\n",
      "Epoch 472/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2499 - accuracy: 0.8906\n",
      "Epoch 472: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2502 - accuracy: 0.8904 - val_loss: 0.6925 - val_accuracy: 0.8101\n",
      "Epoch 473/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2597 - accuracy: 0.8844\n",
      "Epoch 473: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2524 - accuracy: 0.8890 - val_loss: 0.6533 - val_accuracy: 0.8045\n",
      "Epoch 474/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2594 - accuracy: 0.8914\n",
      "Epoch 474: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2626 - accuracy: 0.8862 - val_loss: 0.7047 - val_accuracy: 0.7933\n",
      "Epoch 475/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2689 - accuracy: 0.8914\n",
      "Epoch 475: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2657 - accuracy: 0.8890 - val_loss: 0.7072 - val_accuracy: 0.7933\n",
      "Epoch 476/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2491 - accuracy: 0.8988\n",
      "Epoch 476: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2528 - accuracy: 0.8947 - val_loss: 0.7260 - val_accuracy: 0.8101\n",
      "Epoch 477/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2448 - accuracy: 0.8958\n",
      "Epoch 477: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2567 - accuracy: 0.8919 - val_loss: 0.7503 - val_accuracy: 0.7989\n",
      "Epoch 478/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2527 - accuracy: 0.8854\n",
      "Epoch 478: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2552 - accuracy: 0.8848 - val_loss: 0.7703 - val_accuracy: 0.7989\n",
      "Epoch 479/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2599 - accuracy: 0.8935\n",
      "Epoch 479: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2625 - accuracy: 0.8904 - val_loss: 0.7304 - val_accuracy: 0.8101\n",
      "Epoch 480/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2520 - accuracy: 0.8869\n",
      "Epoch 480: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2565 - accuracy: 0.8862 - val_loss: 0.8051 - val_accuracy: 0.7989\n",
      "Epoch 481/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2436 - accuracy: 0.8973\n",
      "Epoch 481: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2481 - accuracy: 0.8961 - val_loss: 0.7321 - val_accuracy: 0.7933\n",
      "Epoch 482/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2493 - accuracy: 0.8958\n",
      "Epoch 482: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2525 - accuracy: 0.8947 - val_loss: 0.6882 - val_accuracy: 0.8156\n",
      "Epoch 483/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2599 - accuracy: 0.8914\n",
      "Epoch 483: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2578 - accuracy: 0.8947 - val_loss: 0.7876 - val_accuracy: 0.8156\n",
      "Epoch 484/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2554 - accuracy: 0.8988\n",
      "Epoch 484: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2550 - accuracy: 0.8975 - val_loss: 0.7343 - val_accuracy: 0.8156\n",
      "Epoch 485/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2448 - accuracy: 0.8963\n",
      "Epoch 485: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2429 - accuracy: 0.8975 - val_loss: 0.7885 - val_accuracy: 0.7989\n",
      "Epoch 486/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2568 - accuracy: 0.8899\n",
      "Epoch 486: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2576 - accuracy: 0.8933 - val_loss: 0.7526 - val_accuracy: 0.8156\n",
      "Epoch 487/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2415 - accuracy: 0.8991\n",
      "Epoch 487: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2431 - accuracy: 0.8975 - val_loss: 0.7217 - val_accuracy: 0.8045\n",
      "Epoch 488/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2466 - accuracy: 0.8973\n",
      "Epoch 488: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2502 - accuracy: 0.8947 - val_loss: 0.7578 - val_accuracy: 0.8156\n",
      "Epoch 489/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2364 - accuracy: 0.9003\n",
      "Epoch 489: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 15ms/step - loss: 0.2456 - accuracy: 0.8961 - val_loss: 0.7685 - val_accuracy: 0.8045\n",
      "Epoch 490/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2443 - accuracy: 0.8984\n",
      "Epoch 490: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2463 - accuracy: 0.8961 - val_loss: 0.7827 - val_accuracy: 0.8045\n",
      "Epoch 491/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2531 - accuracy: 0.8984\n",
      "Epoch 491: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2557 - accuracy: 0.8989 - val_loss: 0.7700 - val_accuracy: 0.8101\n",
      "Epoch 492/500\n",
      "22/23 [===========================>..] - ETA: 0s - loss: 0.2571 - accuracy: 0.8963\n",
      "Epoch 492: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2578 - accuracy: 0.8947 - val_loss: 0.7674 - val_accuracy: 0.8045\n",
      "Epoch 493/500\n",
      "19/23 [=======================>......] - ETA: 0s - loss: 0.2328 - accuracy: 0.9095\n",
      "Epoch 493: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 0.2338 - accuracy: 0.9087 - val_loss: 0.7771 - val_accuracy: 0.8045\n",
      "Epoch 494/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2380 - accuracy: 0.8884\n",
      "Epoch 494: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2375 - accuracy: 0.8904 - val_loss: 0.7858 - val_accuracy: 0.8156\n",
      "Epoch 495/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2381 - accuracy: 0.9033\n",
      "Epoch 495: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 17ms/step - loss: 0.2347 - accuracy: 0.9059 - val_loss: 0.8021 - val_accuracy: 0.8101\n",
      "Epoch 496/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2362 - accuracy: 0.9033\n",
      "Epoch 496: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2441 - accuracy: 0.9003 - val_loss: 0.7842 - val_accuracy: 0.7933\n",
      "Epoch 497/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2468 - accuracy: 0.8943\n",
      "Epoch 497: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2546 - accuracy: 0.8933 - val_loss: 0.7512 - val_accuracy: 0.8045\n",
      "Epoch 498/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2429 - accuracy: 0.8929\n",
      "Epoch 498: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2440 - accuracy: 0.8933 - val_loss: 0.7565 - val_accuracy: 0.8156\n",
      "Epoch 499/500\n",
      "20/23 [=========================>....] - ETA: 0s - loss: 0.2575 - accuracy: 0.8922\n",
      "Epoch 499: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2505 - accuracy: 0.8975 - val_loss: 0.7745 - val_accuracy: 0.7821\n",
      "Epoch 500/500\n",
      "21/23 [==========================>...] - ETA: 0s - loss: 0.2314 - accuracy: 0.9048\n",
      "Epoch 500: val_loss did not improve from 0.43090\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.2397 - accuracy: 0.9003 - val_loss: 0.7749 - val_accuracy: 0.7989\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "history = model.fit(X_train, y_train, epochs = 500, batch_size = 32, validation_data = (X_val, y_val),\n",
    "                    callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.plot(history.history['loss'], 'b-', label = 'loss')\n",
    "plt.plot(history.history['val_loss'], 'r--', label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_21 (Dense)            (None, 300)               3900      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 300)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 300)               90300     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 300)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 301       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 94,501\n",
      "Trainable params: 94,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# call best model\n",
    "from keras.models import load_model\n",
    "\n",
    "best_model = load_model('best_model.h5')\n",
    "best_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop('Survived', axis = 1)\n",
    "y_test = test.Survived\n",
    "X_test.Age = X_test.Age.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.33372569e-02],\n",
       "       [4.28942731e-05],\n",
       "       [4.52859538e-12],\n",
       "       [1.09929897e-01],\n",
       "       [9.33952749e-01],\n",
       "       [7.54556209e-02],\n",
       "       [1.25694469e-01],\n",
       "       [3.43193533e-03],\n",
       "       [9.99977708e-01],\n",
       "       [1.75230380e-04],\n",
       "       [4.07002196e-02],\n",
       "       [5.63496530e-01],\n",
       "       [9.99999940e-01],\n",
       "       [4.28320542e-02],\n",
       "       [9.99998152e-01],\n",
       "       [9.96273994e-01],\n",
       "       [1.28876290e-03],\n",
       "       [4.47726339e-01],\n",
       "       [6.08226433e-02],\n",
       "       [8.64734320e-05],\n",
       "       [9.07268524e-01],\n",
       "       [9.03001547e-01],\n",
       "       [9.99978423e-01],\n",
       "       [6.80799663e-01],\n",
       "       [6.35340154e-01],\n",
       "       [3.33519615e-02],\n",
       "       [9.99946952e-01],\n",
       "       [1.71090886e-01],\n",
       "       [5.32501340e-01],\n",
       "       [2.03392841e-03],\n",
       "       [3.54171731e-03],\n",
       "       [6.54456764e-03],\n",
       "       [3.46447885e-01],\n",
       "       [5.09664975e-03],\n",
       "       [7.95096457e-01],\n",
       "       [9.91537631e-01],\n",
       "       [6.01053476e-01],\n",
       "       [4.55638289e-01],\n",
       "       [1.61755700e-02],\n",
       "       [7.29876995e-01],\n",
       "       [5.46517521e-02],\n",
       "       [5.16015053e-01],\n",
       "       [4.81293574e-02],\n",
       "       [9.40614164e-01],\n",
       "       [9.99988437e-01],\n",
       "       [7.81939477e-02],\n",
       "       [5.32399476e-01],\n",
       "       [5.75639121e-02],\n",
       "       [9.99999583e-01],\n",
       "       [8.81297067e-02],\n",
       "       [5.34514308e-01],\n",
       "       [7.40742385e-01],\n",
       "       [5.58852851e-01],\n",
       "       [9.32702601e-01],\n",
       "       [3.19610126e-02],\n",
       "       [2.37573730e-07],\n",
       "       [8.89348984e-02],\n",
       "       [1.09654620e-01],\n",
       "       [2.66687968e-03],\n",
       "       [9.86607969e-01],\n",
       "       [7.50859305e-02],\n",
       "       [1.63256571e-01],\n",
       "       [4.83267643e-02],\n",
       "       [7.86169887e-01],\n",
       "       [8.33358526e-01],\n",
       "       [9.97936845e-01],\n",
       "       [8.85359645e-01],\n",
       "       [5.69286458e-02],\n",
       "       [3.71316195e-01],\n",
       "       [5.81299245e-01],\n",
       "       [8.12231660e-01],\n",
       "       [1.62164066e-02],\n",
       "       [4.99910951e-01],\n",
       "       [3.21264803e-01],\n",
       "       [9.99855697e-01],\n",
       "       [2.03943685e-01],\n",
       "       [3.35188322e-02],\n",
       "       [6.22117426e-03],\n",
       "       [1.24966279e-01],\n",
       "       [8.12231660e-01],\n",
       "       [9.99999702e-01],\n",
       "       [9.49930131e-01],\n",
       "       [4.21628684e-01],\n",
       "       [4.07002196e-02],\n",
       "       [3.82137462e-03],\n",
       "       [3.41744907e-02],\n",
       "       [6.11361265e-01],\n",
       "       [4.26380217e-01],\n",
       "       [8.12231660e-01],\n",
       "       [1.00000000e+00],\n",
       "       [4.27819997e-01],\n",
       "       [4.77623791e-02],\n",
       "       [9.93850410e-01],\n",
       "       [3.35188322e-02],\n",
       "       [2.96338081e-01],\n",
       "       [9.34628248e-02],\n",
       "       [9.99997377e-01],\n",
       "       [1.25647485e-01],\n",
       "       [6.08895421e-01],\n",
       "       [1.02398500e-01],\n",
       "       [9.99986768e-01],\n",
       "       [1.45348281e-01],\n",
       "       [5.75639121e-02],\n",
       "       [1.33468017e-01],\n",
       "       [8.97860587e-01],\n",
       "       [1.21711856e-02],\n",
       "       [1.08083952e-02],\n",
       "       [5.75639121e-02],\n",
       "       [1.78796444e-02],\n",
       "       [5.74598387e-02],\n",
       "       [5.74081112e-03],\n",
       "       [8.13548923e-01],\n",
       "       [9.98678625e-01],\n",
       "       [6.20600879e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.52650011e-01],\n",
       "       [1.43179312e-01],\n",
       "       [9.77295399e-01],\n",
       "       [1.01193964e-01],\n",
       "       [9.92644489e-01],\n",
       "       [9.99530494e-01],\n",
       "       [1.99778867e-03],\n",
       "       [9.99995947e-01],\n",
       "       [1.24019831e-01],\n",
       "       [5.75639121e-02],\n",
       "       [3.37173939e-01],\n",
       "       [1.68019384e-02],\n",
       "       [9.41856146e-01],\n",
       "       [2.08403990e-02],\n",
       "       [3.35188322e-02],\n",
       "       [1.08019106e-01],\n",
       "       [3.34347449e-02],\n",
       "       [6.97635651e-01],\n",
       "       [3.06167756e-04],\n",
       "       [3.84411477e-02],\n",
       "       [4.29114737e-02],\n",
       "       [1.42378956e-01],\n",
       "       [8.37489888e-02],\n",
       "       [5.98368943e-01],\n",
       "       [2.74673852e-07],\n",
       "       [2.88484851e-04],\n",
       "       [1.00000000e+00],\n",
       "       [1.56214580e-01],\n",
       "       [2.62044035e-02],\n",
       "       [6.83383584e-01],\n",
       "       [2.86655617e-03],\n",
       "       [1.22961275e-01],\n",
       "       [1.61102712e-02],\n",
       "       [5.16015053e-01],\n",
       "       [1.06293494e-02],\n",
       "       [9.99988735e-01],\n",
       "       [1.78367436e-01],\n",
       "       [           nan],\n",
       "       [1.76734865e-01],\n",
       "       [1.36188760e-06],\n",
       "       [6.57756329e-02],\n",
       "       [9.39055204e-01],\n",
       "       [5.97171009e-01],\n",
       "       [6.83383584e-01],\n",
       "       [9.36287284e-01],\n",
       "       [8.11306775e-01],\n",
       "       [9.99999464e-01],\n",
       "       [6.11495733e-01],\n",
       "       [1.20083183e-01],\n",
       "       [2.67447159e-02],\n",
       "       [2.25877520e-02],\n",
       "       [3.34855437e-01],\n",
       "       [6.76846757e-05],\n",
       "       [9.99520063e-01],\n",
       "       [5.05057216e-01],\n",
       "       [6.57756329e-02],\n",
       "       [1.32742375e-01],\n",
       "       [3.99243739e-03],\n",
       "       [1.64297134e-01],\n",
       "       [3.64818443e-05],\n",
       "       [9.99636948e-01],\n",
       "       [9.99994993e-01],\n",
       "       [8.95345151e-01],\n",
       "       [1.52883723e-01],\n",
       "       [9.99189556e-01],\n",
       "       [1.24966279e-01],\n",
       "       [7.44684815e-01],\n",
       "       [9.99998391e-01],\n",
       "       [5.75639121e-02],\n",
       "       [9.84358013e-01],\n",
       "       [1.67347863e-01],\n",
       "       [9.82717752e-01],\n",
       "       [1.75310497e-03],\n",
       "       [7.94178473e-11],\n",
       "       [3.47738154e-02],\n",
       "       [1.52317867e-01],\n",
       "       [5.16464651e-01],\n",
       "       [9.41213369e-01],\n",
       "       [2.08675499e-09],\n",
       "       [1.00000000e+00],\n",
       "       [1.00632191e-01],\n",
       "       [9.27663982e-01],\n",
       "       [4.66656446e-01],\n",
       "       [1.28531188e-01],\n",
       "       [6.01883292e-01],\n",
       "       [9.99499142e-01],\n",
       "       [1.00000000e+00],\n",
       "       [2.06626996e-01],\n",
       "       [9.99994993e-01],\n",
       "       [1.54570431e-01],\n",
       "       [2.88377047e-01],\n",
       "       [3.19011509e-02],\n",
       "       [1.42657518e-01],\n",
       "       [9.96772110e-01],\n",
       "       [8.19265693e-02],\n",
       "       [1.13369953e-02],\n",
       "       [1.16578177e-01],\n",
       "       [1.29840616e-02],\n",
       "       [9.08139348e-01],\n",
       "       [1.40174968e-18],\n",
       "       [8.75325501e-02],\n",
       "       [8.17910850e-01],\n",
       "       [3.66199523e-01],\n",
       "       [9.99999225e-01],\n",
       "       [3.35188322e-02],\n",
       "       [9.83275950e-01],\n",
       "       [1.61814056e-02],\n",
       "       [9.56689000e-01],\n",
       "       [1.64844859e-02],\n",
       "       [5.74313626e-02],\n",
       "       [1.05102900e-02],\n",
       "       [2.50430889e-02],\n",
       "       [8.12231660e-01],\n",
       "       [5.71916997e-02],\n",
       "       [1.00148179e-01],\n",
       "       [6.97442219e-02],\n",
       "       [9.99958217e-01],\n",
       "       [1.06136583e-01],\n",
       "       [5.04295379e-02],\n",
       "       [8.33757818e-01],\n",
       "       [1.83369797e-02],\n",
       "       [8.56046379e-01],\n",
       "       [8.57722104e-01],\n",
       "       [9.99997437e-01],\n",
       "       [1.00000000e+00],\n",
       "       [9.57349613e-02],\n",
       "       [9.89955068e-01],\n",
       "       [4.58096981e-01],\n",
       "       [4.11192067e-02],\n",
       "       [1.54170251e-04],\n",
       "       [4.87116247e-01],\n",
       "       [9.98336315e-01],\n",
       "       [2.67897118e-02],\n",
       "       [9.92644489e-01],\n",
       "       [1.13415066e-02],\n",
       "       [1.00000000e+00],\n",
       "       [1.86268091e-02],\n",
       "       [2.52977639e-01],\n",
       "       [1.59963518e-02],\n",
       "       [1.21609285e-01],\n",
       "       [6.57756329e-02],\n",
       "       [5.75639121e-02],\n",
       "       [1.32815689e-01],\n",
       "       [9.62310612e-01],\n",
       "       [1.65407937e-02],\n",
       "       [5.40465396e-03],\n",
       "       [1.63274016e-02],\n",
       "       [9.93068039e-01],\n",
       "       [9.98124123e-01],\n",
       "       [1.34527072e-01],\n",
       "       [4.07002196e-02],\n",
       "       [2.35949091e-08],\n",
       "       [6.57756329e-02],\n",
       "       [6.01053476e-01],\n",
       "       [6.51805401e-02],\n",
       "       [1.04507111e-01],\n",
       "       [5.75639121e-02],\n",
       "       [9.99993026e-01],\n",
       "       [9.96321797e-01],\n",
       "       [1.64214805e-01],\n",
       "       [9.98324215e-01],\n",
       "       [1.13006845e-01],\n",
       "       [2.01897845e-02],\n",
       "       [7.97868818e-02],\n",
       "       [1.41976476e-01],\n",
       "       [5.94408274e-01],\n",
       "       [9.99991596e-01],\n",
       "       [8.12231660e-01],\n",
       "       [9.45682347e-01],\n",
       "       [8.76684189e-01],\n",
       "       [7.40211755e-02],\n",
       "       [9.71131101e-02],\n",
       "       [4.05074686e-01],\n",
       "       [1.64297134e-01],\n",
       "       [3.35188322e-02],\n",
       "       [2.38828108e-01],\n",
       "       [7.40841702e-02],\n",
       "       [1.64297134e-01],\n",
       "       [2.65440553e-01],\n",
       "       [9.96888205e-02],\n",
       "       [1.30913764e-01],\n",
       "       [1.00000000e+00],\n",
       "       [2.03392841e-03],\n",
       "       [1.46473438e-01],\n",
       "       [1.24723487e-01],\n",
       "       [1.05493516e-01],\n",
       "       [6.06461763e-02],\n",
       "       [1.78789478e-02],\n",
       "       [1.83672775e-02],\n",
       "       [8.12231660e-01],\n",
       "       [9.79340792e-01],\n",
       "       [2.53457665e-01],\n",
       "       [1.00000000e+00],\n",
       "       [1.94840252e-01],\n",
       "       [2.03475002e-02],\n",
       "       [4.65870351e-02],\n",
       "       [1.76087230e-01],\n",
       "       [6.34813532e-02],\n",
       "       [1.73565596e-02],\n",
       "       [1.00000000e+00],\n",
       "       [9.59418654e-01],\n",
       "       [4.80985671e-01],\n",
       "       [2.35375434e-01],\n",
       "       [1.36753052e-01],\n",
       "       [2.91232509e-03],\n",
       "       [1.33468017e-01],\n",
       "       [1.58741295e-01],\n",
       "       [8.37489888e-02],\n",
       "       [6.90662742e-01],\n",
       "       [9.99929309e-01],\n",
       "       [6.91463202e-02],\n",
       "       [9.67728555e-01],\n",
       "       [7.56751150e-02],\n",
       "       [2.19361514e-01],\n",
       "       [1.04128800e-01],\n",
       "       [9.85763192e-01],\n",
       "       [5.81002295e-01],\n",
       "       [1.64214805e-01],\n",
       "       [7.01606274e-01],\n",
       "       [1.36909276e-01],\n",
       "       [6.56472266e-01],\n",
       "       [1.74716011e-01],\n",
       "       [1.30918977e-07],\n",
       "       [1.50741078e-02],\n",
       "       [1.64297134e-01],\n",
       "       [2.39491358e-01],\n",
       "       [1.03841573e-01],\n",
       "       [9.76423770e-02],\n",
       "       [9.99272704e-01],\n",
       "       [1.98866194e-03],\n",
       "       [4.82084632e-01],\n",
       "       [8.37489888e-02],\n",
       "       [1.80170289e-03],\n",
       "       [1.05276853e-01],\n",
       "       [9.70302343e-01],\n",
       "       [9.99930799e-01],\n",
       "       [1.54570431e-01],\n",
       "       [1.01072481e-02],\n",
       "       [2.77948275e-04],\n",
       "       [9.93928969e-01],\n",
       "       [3.69916171e-01],\n",
       "       [9.99867141e-01],\n",
       "       [4.15422730e-02],\n",
       "       [5.75639121e-02],\n",
       "       [2.20334735e-02],\n",
       "       [2.07561413e-07],\n",
       "       [9.93589044e-01],\n",
       "       [9.70302343e-01],\n",
       "       [1.09929897e-01],\n",
       "       [9.99994993e-01],\n",
       "       [2.70735417e-02],\n",
       "       [3.43707576e-02],\n",
       "       [9.99878645e-01],\n",
       "       [9.99873221e-01],\n",
       "       [6.48749650e-01],\n",
       "       [7.51849450e-03],\n",
       "       [1.00000000e+00],\n",
       "       [1.24362909e-13],\n",
       "       [1.32453823e-02],\n",
       "       [9.99998927e-01],\n",
       "       [9.99926865e-01],\n",
       "       [9.76330042e-01],\n",
       "       [1.45389438e-01],\n",
       "       [1.40748117e-02],\n",
       "       [1.86349943e-01],\n",
       "       [5.75639121e-02],\n",
       "       [1.33242667e-01],\n",
       "       [6.98412418e-01],\n",
       "       [7.29403615e-01],\n",
       "       [1.02181263e-01],\n",
       "       [9.99232411e-01],\n",
       "       [4.77623791e-02],\n",
       "       [4.69026272e-05],\n",
       "       [1.13097709e-02],\n",
       "       [1.03685097e-03],\n",
       "       [2.88792491e-01],\n",
       "       [9.99165535e-01],\n",
       "       [9.85007107e-01],\n",
       "       [8.82870881e-05],\n",
       "       [5.91315256e-05],\n",
       "       [9.99999285e-01],\n",
       "       [9.56227407e-02],\n",
       "       [9.99991596e-01],\n",
       "       [1.69910509e-02],\n",
       "       [1.63814604e-01],\n",
       "       [9.99958992e-01],\n",
       "       [1.28215700e-02],\n",
       "       [9.99930799e-01],\n",
       "       [3.19063872e-01],\n",
       "       [2.31650367e-01],\n",
       "       [3.73605974e-02],\n",
       "       [1.37419777e-03],\n",
       "       [2.45354518e-01],\n",
       "       [8.10612619e-01],\n",
       "       [9.92742181e-01],\n",
       "       [8.12231660e-01],\n",
       "       [1.00000000e+00],\n",
       "       [5.37292838e-01],\n",
       "       [3.35188322e-02],\n",
       "       [1.00000000e+00],\n",
       "       [5.64915463e-02],\n",
       "       [3.35188322e-02],\n",
       "       [9.07842159e-01]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.where(y_pred > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8301435406698564"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "896dec041192bff481558047fad7d0e197250d6ee1a43bc027cd6e5b5fb3edf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
