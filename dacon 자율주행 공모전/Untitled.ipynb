{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff288cb8-a485-44d5-a018-a2cace5b5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 코드\n",
    "# light gbm 사용하기\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "dataset = pd.read_csv('train.csv')\n",
    "X = dataset.iloc[:,1:57]\n",
    "y = dataset.iloc[:,57:]\n",
    "\n",
    "X.drop(['X_04', 'X_10', 'X_11', 'X_23', 'X_47', 'X_48', 'X_37'], axis = 1, inplace = True) \n",
    "\n",
    "sc = MinMaxScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "# 훈련데이터, 테스트 데이터 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "\n",
    "# 평가방식\n",
    "from sklearn import metrics\n",
    "def lg_nrmse(gt, preds):\n",
    "    # 각 Y Feature별 NRMSE 총합\n",
    "    # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "    all_nrmse = []\n",
    "    for idx in range(0,14): # ignore 'ID'\n",
    "        rmse = metrics.mean_squared_error(gt[:,idx], preds[:,idx], squared=False)\n",
    "        nrmse = rmse/np.mean(np.abs(gt[:,idx]))\n",
    "        all_nrmse.append(nrmse)\n",
    "    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:14])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f65e065b-6e02-404f-ad5c-bf85868acbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.009977115814443\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso()\n",
    "lasso.fit(x_train, y_train)\n",
    "lasso_preds = lasso.predict(x_test)\n",
    "y_test = np.array(y_test)\n",
    "print(lg_nrmse(y_test, lasso_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a450b7d2-9844-4d80-9037-8d11c91082ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.014258847055671\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sg = MultiOutputRegressor(SGDRegressor(max_iter = 1000, random_state = 0))\n",
    "sg.fit(x_train, y_train)\n",
    "sg_preds = sg.predict(x_test)\n",
    "y_test = np.array(y_test)\n",
    "print(lg_nrmse(y_test, sg_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa470c27-e5fc-41ba-8832-a2a5a03ba975",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lightgbm 옵션 변경해보기\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm\n",
    "lgb_reg = MultiOutputRegressor(LGBMRegressor(n_estimators = 300, learning_rate = 0.03)).fit(X, y)\n",
    "lgb_preds = lgb_reg.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88e05bae-8171-444c-b609-68ccd964296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submit = pd.read_csv('sample_submission.csv')\n",
    "for idx, col in enumerate(submit.columns):\n",
    "    if col=='ID':\n",
    "        continue\n",
    "    submit[col] = lgb_preds[:,idx-1]\n",
    "submit.to_csv('D:/자율주행ai/submit0818_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59dba7fe-5022-4353-8ed2-a82d949f6620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kgt08\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "xgb_reg = MultiOutputRegressor(XGBRegressor(n_estimators = 300, learning_rate = 0.03)).fit(x_train, y_train)\n",
    "xgb_preds = xgb_reg.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc648e67-27e8-439c-8cbf-9c22318fa15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 포레스트\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "rdm = MultiOutputRegressor(RandomForestRegressor(n_estimators = 100, min_samples_leaf = 8) ).fit(x_train, y_train) #이거는 한 20분걸림 \n",
    "preds_rdm = rdm.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1eabf30-4938-4fbf-ac44-313461f34598",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "test.drop(['X_04', 'X_10', 'X_11', 'X_23', 'X_47', 'X_48', 'X_37', 'ID'], axis = 1, inplace = True) \n",
    "test = sc.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dc41696b-2200-42ed-b44f-c29cfaa93e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39608, 49) (39607, 14)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e99172e2-f301-438f-9248-7b68d39e83e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.42855141, 1.        , 0.617737  , ..., 0.70502544, 0.74847877,\n",
       "        0.75036626],\n",
       "       [0.39286215, 0.        , 0.39449541, ..., 0.72259053, 0.75181443,\n",
       "        0.7676889 ],\n",
       "       [0.46427571, 0.        , 0.382263  , ..., 0.70723872, 0.71528637,\n",
       "        0.69520684],\n",
       "       ...,\n",
       "       [0.46427571, 0.        , 0.22018349, ..., 0.68137123, 0.76558778,\n",
       "        0.79377801],\n",
       "       [0.39286215, 1.        , 0.16207951, ..., 0.70232827, 0.74532527,\n",
       "        0.82524034],\n",
       "       [0.53568927, 0.        , 0.20183486, ..., 0.71833873, 0.79444622,\n",
       "        0.74752395]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c72db8b9-f1f7-4bea-9c93-14777dad68b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyeongtae\\AppData\\Local\\Temp\\ipykernel_1568\\4064839527.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X = X.append(add, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "add = X.iloc[-1,:]\n",
    "X = X.append(add, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e92d5db-3af7-444c-9712-0102adb44455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.42855141, 1.        , 0.617737  , ..., 0.70502544, 0.74847877,\n",
       "        0.75036626],\n",
       "       [0.39286215, 0.        , 0.39449541, ..., 0.72259053, 0.75181443,\n",
       "        0.7676889 ],\n",
       "       [0.46427571, 0.        , 0.382263  , ..., 0.70723872, 0.71528637,\n",
       "        0.69520684],\n",
       "       ...,\n",
       "       [0.46427571, 0.        , 0.22018349, ..., 0.68137123, 0.76558778,\n",
       "        0.79377801],\n",
       "       [0.39286215, 1.        , 0.16207951, ..., 0.70232827, 0.74532527,\n",
       "        0.82524034],\n",
       "       [0.53568927, 0.        , 0.20183486, ..., 0.71833873, 0.79444622,\n",
       "        0.74752395]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test.drop(['X_37', 'ID'], axis = 1, inplace = True) \n",
    "test = sc.transform(test);test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e9cfe588-2a59-4077-a935-c01ea50106cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyeongtae\\AppData\\Local\\Temp\\ipykernel_1568\\2552258699.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X = X.append(add, ignore_index = True)\n",
      "C:\\Users\\kyeongtae\\AppData\\Local\\Temp\\ipykernel_1568\\2552258699.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y = y.append(addy, ignore_index = True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous-multioutput'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [92]\u001b[0m, in \u001b[0;36m<cell line: 68>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VotingClassifier\n\u001b[0;32m     66\u001b[0m voting_clf \u001b[38;5;241m=\u001b[39m VotingClassifier(\n\u001b[0;32m     67\u001b[0m     estimators \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m, lgb_reg), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxr\u001b[39m\u001b[38;5;124m'\u001b[39m, xgb_reg), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrr\u001b[39m\u001b[38;5;124m'\u001b[39m, rdm)], voting \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m \u001b[43mvoting_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m vot_preds \u001b[38;5;241m=\u001b[39m voting_clf\u001b[38;5;241m.\u001b[39mpredict(test)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py:309\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit the estimators.\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    312\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultilabel and multi-output classification is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m         )\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py:197\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    189\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m ]:\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'continuous-multioutput'"
     ]
    }
   ],
   "source": [
    "# 최적화 코드\n",
    "# light gbm 사용하기\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "dataset = pd.read_csv('train.csv')\n",
    "X = dataset.iloc[:,1:57]\n",
    "y = dataset.iloc[:,57:]\n",
    "add = X.iloc[-1,:]\n",
    "X = X.append(add, ignore_index = True)\n",
    "addy = y.iloc[-1,:]\n",
    "y = y.append(addy, ignore_index = True)\n",
    "X.drop(['X_04', 'X_10', 'X_11', 'X_23', 'X_47', 'X_48', 'X_37'], axis = 1, inplace = True) \n",
    "sc = StandardScaler()\n",
    "sc.fit(X)\n",
    "X = sc.transform(X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 훈련데이터, 테스트 데이터 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "\n",
    "# 평가방식\n",
    "from sklearn import metrics\n",
    "def lg_nrmse(gt, preds):\n",
    "    # 각 Y Feature별 NRMSE 총합\n",
    "    # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "    all_nrmse = []\n",
    "    for idx in range(0,14): # ignore 'ID'\n",
    "        rmse = metrics.mean_squared_error(gt[:,idx], preds[:,idx], squared=False)\n",
    "        nrmse = rmse/np.mean(np.abs(gt[:,idx]))\n",
    "        all_nrmse.append(nrmse)\n",
    "    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:14])\n",
    "    return score\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "test.drop(['X_04', 'X_10', 'X_11', 'X_23', 'X_47', 'X_48', 'X_37', 'ID'], axis = 1, inplace = True) \n",
    "test = sc.transform(test)\n",
    "\n",
    "\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm\n",
    "lgb_reg = MultiOutputRegressor(LGBMRegressor(n_estimators = 300, learning_rate = 0.03)).fit(X, y)\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "xgb_reg = MultiOutputRegressor(XGBRegressor(n_estimators = 300, learning_rate = 0.03)).fit(X, y)\n",
    "\n",
    "# 랜덤 포레스트\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "rdm = MultiOutputRegressor(RandomForestRegressor(n_estimators = 100, min_samples_leaf = 8) ).fit(X, y) #이거는 한 20분걸림 \n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', lgb_reg), ('xr', xgb_reg), ('rr', rdm)], voting = 'soft')\n",
    "voting_clf.fit(x,y)\n",
    "vot_preds = voting_clf.predict(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4ed31021-539b-4fb7-a233-5abecd70806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyeongtae\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\kyeongtae\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The estimator MultiOutputRegressor should be a classifier.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [100]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m y_train \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mvoting_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m vot_preds \u001b[38;5;241m=\u001b[39m voting_clf\u001b[38;5;241m.\u001b[39mpredict(test)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py:324\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m    322\u001b[0m transformed_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle_\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py:65\u001b[0m, in \u001b[0;36m_BaseVoting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;129m@abstractmethod\u001b[39m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;124;03m\"\"\"Get common fit operations.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m     names, clfs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_estimators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators):\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of `estimators` and weights must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m; got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m weights, \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m             \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators))\n\u001b[0;32m     72\u001b[0m         )\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_base.py:262\u001b[0m, in \u001b[0;36m_BaseHeterogeneousEnsemble._validate_estimators\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m est \u001b[38;5;129;01min\u001b[39;00m estimators:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_estimator_type(est):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    263\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe estimator \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m should be a \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    264\u001b[0m                 est\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_estimator_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m[\u001b[38;5;241m3\u001b[39m:]\n\u001b[0;32m    265\u001b[0m             )\n\u001b[0;32m    266\u001b[0m         )\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m names, estimators\n",
      "\u001b[1;31mValueError\u001b[0m: The estimator MultiOutputRegressor should be a classifier."
     ]
    }
   ],
   "source": [
    "y_train = y_train.astype('int')\n",
    "voting_clf.fit(x_train,np.array(y_train).reshape(-1,1))\n",
    "vot_preds = voting_clf.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "359afc81-5ddf-4ab4-87d1-7bf2876c072b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.034643  ,  2.23898549,  1.52273986, ...,  0.31236162,\n",
       "        -0.7244662 , -1.20891282],\n",
       "       [-0.34902625, -0.44663085,  0.10555612, ...,  0.8092448 ,\n",
       "        -0.64953742, -0.78297134],\n",
       "       [ 0.41868875, -0.44663085,  0.02790222, ...,  0.37497116,\n",
       "        -1.47006557, -2.56521227],\n",
       "       ...,\n",
       "       [ 0.41868875, -0.44663085, -1.00101201, ..., -0.35677077,\n",
       "        -0.34014718, -0.14147357],\n",
       "       [-0.34902625,  2.23898549, -1.36986805, ...,  0.23606396,\n",
       "        -0.79530306,  0.63214484],\n",
       "       [ 1.18640376, -0.44663085, -1.11749286, ...,  0.68896938,\n",
       "         0.30809862, -1.27880157]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c296158-aa31-42f2-b944-9bbbe4d1d332",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaggingRegressor\n\u001b[1;32m----> 2\u001b[0m bagging_model \u001b[38;5;241m=\u001b[39m BaggingRegressor(base_estimator \u001b[38;5;241m=\u001b[39m [lgb_reg, \u001b[43mxgb_reg\u001b[49m, rdm], \u001b[38;5;66;03m# 선형회귀모형\u001b[39;00m\n\u001b[0;32m      3\u001b[0m                                  n_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;66;03m# 5개의 샘플링\u001b[39;00m\n\u001b[0;32m      4\u001b[0m                                  verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# 학습 과정 표시\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model2 \u001b[38;5;241m=\u001b[39m bagging_model\u001b[38;5;241m.\u001b[39mfit(x_train, y_train) \u001b[38;5;66;03m# 학습 진행\u001b[39;00m\n\u001b[0;32m      6\u001b[0m predict2 \u001b[38;5;241m=\u001b[39m model2\u001b[38;5;241m.\u001b[39mpredict(x_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xgb_reg' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "bagging_model = BaggingRegressor(base_estimator = [lgb_reg, xgb_reg, rdm], # 선형회귀모형\n",
    "                                 n_estimators = 5, # 5개의 샘플링\n",
    "                                 verbose = 1) # 학습 과정 표시\n",
    "model2 = bagging_model.fit(x_train, y_train) # 학습 진행\n",
    "predict2 = model2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67daaaa0-ea1f-43f2-9a6b-1ba7ec333a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스태킹방법\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수. \n",
    "def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds ):\n",
    "    # 지정된 n_folds값으로 KFold 생성.\n",
    "    kf = KFold(n_splits=n_folds, shuffle=False, random_state=0)\n",
    "    #추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화 \n",
    "    train_fold_pred = np.zeros((X_train_n.shape[0] ,1 ))\n",
    "    test_pred = np.zeros((X_test_n.shape[0],n_folds))\n",
    "    print(model.__class__.__name__ , ' model 시작 ')\n",
    "    \n",
    "    for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n",
    "        #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출 \n",
    "        print('\\t 폴드 세트: ',folder_counter,' 시작 ')\n",
    "        X_tr = X_train_n[train_index] \n",
    "        y_tr = y_train_n[train_index] \n",
    "        X_te = X_train_n[valid_index]  \n",
    "        \n",
    "        #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행.\n",
    "        model.fit(X_tr , y_tr)       \n",
    "        #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장.\n",
    "        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1)\n",
    "        #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장. \n",
    "        test_pred[:, folder_counter] = model.predict(X_test_n)\n",
    "            \n",
    "    # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성 \n",
    "    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1)    \n",
    "    \n",
    "    #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터\n",
    "    return train_fold_pred , test_pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2265065b-15b8-4fd7-91dd-06db9444eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft voting, stacking 이용해서 다시 해보기\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "models = [('lgb_reg', lgb_reg), ('xgb_reg', xgb_reg), ('rdm', rdm)]\n",
    "voting_regressor = VotingRegressor(single_models, voting = 'soft', n_jobs=-1)\n",
    "voting_regressor.fit(x_train, y_train)\n",
    "voting_pred = voting_regressor.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64506cd-15fe-429c-8997-115e326968f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 코드\n",
    "# light gbm 사용하기\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "dataset = pd.read_csv('train.csv')\n",
    "X = dataset.iloc[:,1:57]\n",
    "y = dataset.iloc[:,57:]\n",
    "\n",
    "X.drop(['X_37'], axis = 1, inplace = True) \n",
    "\n",
    "sc = MinMaxScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "# 훈련데이터, 테스트 데이터 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "\n",
    "\n",
    "# 평가방식\n",
    "from sklearn import metrics\n",
    "def lg_nrmse(gt, preds):\n",
    "    # 각 Y Feature별 NRMSE 총합\n",
    "    # Y_01 ~ Y_08 까지 20% 가중치 부여\n",
    "    all_nrmse = []\n",
    "    for idx in range(0,14): # ignore 'ID'\n",
    "        rmse = metrics.mean_squared_error(gt[:,idx], preds[:,idx], squared=False)\n",
    "        nrmse = rmse/np.mean(np.abs(gt[:,idx]))\n",
    "        all_nrmse.append(nrmse)\n",
    "    score = 1.2 * np.sum(all_nrmse[:8]) + 1.0 * np.sum(all_nrmse[8:14])\n",
    "    return score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수. \n",
    "def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds ):\n",
    "    # 지정된 n_folds값으로 KFold 생성.\n",
    "    kf = KFold(n_splits=n_folds, shuffle=False, random_state=0)\n",
    "    #추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화 \n",
    "    train_fold_pred = np.zeros((X_train_n.shape[0] ,1 ))\n",
    "    test_pred = np.zeros((X_test_n.shape[0],n_folds))\n",
    "    print(model.__class__.__name__ , ' model 시작 ')\n",
    "    \n",
    "    for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n",
    "        #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출 \n",
    "        print('\\t 폴드 세트: ',folder_counter,' 시작 ')\n",
    "        X_tr = X_train_n[train_index] \n",
    "        y_tr = y_train_n[train_index] \n",
    "        X_te = X_train_n[valid_index]  \n",
    "        \n",
    "        #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행.\n",
    "        model.fit(X_tr , y_tr)       \n",
    "        #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장.\n",
    "        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1)\n",
    "        #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장. \n",
    "        test_pred[:, folder_counter] = model.predict(X_test_n)\n",
    "            \n",
    "    # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성 \n",
    "    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1)    \n",
    "    \n",
    "    #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터\n",
    "    return train_fold_pred , test_pred_mean\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm\n",
    "lgb_reg = MultiOutputRegressor(LGBMRegressor(n_estimators = 300, learning_rate = 0.03)).fit(x_train, y_train)\n",
    "\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "xgb_reg = MultiOutputRegressor(XGBRegressor(n_estimators = 300, learning_rate = 0.03)).fit(x_train, y_train)\n",
    "\n",
    "# 랜덤 포레스트\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "rdm = MultiOutputRegressor(RandomForestRegressor(n_estimators = 100, min_samples_leaf = 8)).fit(x_train, y_train) #이거는 한 20분걸림 \n",
    "\n",
    "models = [('lgb_reg', lgb_reg), ('xgb_reg', xgb_reg), ('rdm', rdm)]\n",
    "voting_regressor = VotingRegressor(models, n_jobs=-1)\n",
    "voting_regressor.fit(x_train, y_train)\n",
    "voting_pred = voting_regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b88fe5-dd7d-42de-bbb9-01eeaef3317c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
